{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 2)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAAHfCAYAAAD3H2TtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAH3JJREFUeJzt3XuwXnV97/HPN4aQCzYj5BBpuLQeD1gCVqQGkCPsSrHg\nUfRUBSmnimdGO7aMOJ4pXlsznYK1M21P1TpcFKdaW3VABVIpIcCuUkRBoVIMAvUQEGloS6BCCOTy\nO39kwwTYO7f95Lcveb1m9vA8z/o9z/qxZmXnnbXWXrtaawEAoJ8ZEz0BAIDdjQADAOhMgAEAdCbA\nAAA6E2AAAJ0JMACAzsYdYFW1f1VdW1W3V9VtVfWeMcZ9oqruqqpbq+pl410vAMBUNXMAn7Ehyfta\na7dW1V5JvldVy1trdzw1oKpOTvJfW2v/raqOSnJ+kqMHsG4AgCln3EfAWmv/2lq7deTxo0lWJln0\nrGFvSPL5kTHfSTK/qhaOd90AAFPRQK8Bq6pfSPKyJN951qJFSe7b4vn9eW6kAQDsFgZxCjJJMnL6\n8ZIkZ48cCdvZz/G7kQCAKaO1Vjv6noEcAauqmdkcX19orV02ypD7kxywxfP9R14bVWvNV8evj370\noxM+h93tyza3zXeHL9vcNt8dvnbWoE5BXpzkh621vxhj+eVJ3pYkVXV0kodba6sHtG4AgCll3Kcg\nq+rYJGckua2qbknSknwoyUFJWmvtwtbaN6rqtVV1d5LHkrxjvOsFAJiqxh1grbV/TPK87Rh31njX\nxa4xNDQ00VPY7djm/dnm/dnm/dnmU0eN5/zlrlBVbbLNCQBgNFWVNlEX4QMAsP0EGABAZwIMAKAz\nAQYA0JkAAwDoTIABAHQmwAAAOhNgAACdCTAAgM4EGABAZwIMAKAzAQYA0JkAAwDoTIABAHQmwAAA\nOhNgAACdCTAAgM4EGABAZwIMAKAzAQYA0JkAAwDoTIABAHQmwAAAOhNgAACdCTAAgM4EGABAZwIM\nAKAzAQYA0JkAAwDoTIABAHQmwAAAOhNgAACdCTAAgM4EGABAZwIMAKAzAQYA0JkAAwDoTIABAHQm\nwAAAOhNgAACdCTAAgM4EGABAZwIMAKAzAQYA0JkAAwDoTIABAHQmwAAAOhNgAACdCTAAgM4EGABA\nZwIMAKAzAQYA0JkAAwDoTIABAHQmwAAAOhNgAACdCTAAgM4EGABAZwIMAKAzAQYA0JkAAwDoTIAB\nAHQmwAAAOhNgAACdCTAAgM4GEmBV9dmqWl1VPxhj+fFV9XBVfX/k6yODWC8AwFQ0c0Cf87kkn0zy\n+a2M+WZr7ZQBrQ8AYMoayBGw1tr1SdZsY1gNYl0AAFNdz2vAjqmqW6vq76rq0I7rBQCYVAZ1CnJb\nvpfkwNba2qo6OcnXkxw81uClS5c+/XhoaChDQ0O7en4AANs0PDyc4eHhcX9OtdbGP5skVXVQkita\nay/djrH/L8mRrbWHRlnWBjUnAIBdqarSWtvhy6wGeQqyMsZ1XlW1cIvHS7I5/J4TXwAAu4OBnIKs\nqr9JMpRkn6q6N8lHk8xK0lprFyZ5c1W9O8n6JI8nOW0Q6wUAmIoGdgpyUJyCBACmislwChIAgO0g\nwAAAOhNgAACdCTAAgM4EGABAZwIMAKAzAQYA0JkAAwDoTIABAHQmwAAAOhNgAACdCTAAgM4EGABA\nZwIMAKAzAQYA0JkAAwDoTIABAHQmwAAAOhNgAACdCTAAgM4EGABAZwIMAKAzAQYA0JkAAwDoTIAB\nAHQmwAAAOhNgAACdCTAAgM4EGABAZwIMAKAzAQYA0JkAAwDoTIABAHQmwAAAOhNgAACdCTAAgM4E\nGABAZwIMAKAzAQYA0JkAAwDoTIABAHQmwAAAOhNgAACdCTAAgM4EGABAZwIMAKAzAQYA0JkAAwDo\nTIABAHQmwAAAOhNgAACdCTAAgM4EGABAZwIMAKAzAQYA0JkAAwDoTIABAHQmwAAAOhNgAACdCTAA\ngM4EGABAZwIMAKAzAQYA0JkAAwDobOZETwB2B621XHPNNfnc5z6X++67LwsWLMgZZ5yRU045JXvs\nscdETw+Azqq1Nv4PqfpsktclWd1ae+kYYz6R5OQkjyU5s7V26xjj2iDmBJPFunXrctppp+Xuu+/O\n7/7u7+bQQw/NvffemwsuuCBPPvlkrrzyyixYsGCipwnATqiqtNZqh983oAD770keTfL50QKsqk5O\nclZr7X9U1VFJ/qK1dvQYnyXAmFZ+53d+J6tXr87f/u3fZtasWU+/3lrLOeeck1tvvTVXX331BM4Q\ngJ01oQE2MoGDklwxRoCdn+S61tqXR56vTDLUWls9ylgBxrTx7//+73nxi1+cu+++e9SjXBs2bMiL\nXvSiXHbZZTniiCMmYIYAjMfOBlivi/AXJblvi+f3j7wG09qKFSsyNDQ05inGmTNn5vTTT8/ll1/e\neWYATKRJeRH+0qVLn348NDSUoaGhCZsLjMe6desyf/78rY6ZP39+fvazn3WaEQDjMTw8nOHh4XF/\nzkSdgrwjyfFOQTLdfe9738tb3vKW3HXXXXne85436pjXvOY1ecc73pHTTz+98+wAGK/JcAqyRr5G\nc3mStyVJVR2d5OHR4gummyOPPDL77LNPvvjFL466/Nvf/nb+6Z/+Kb/xG7/ReWYATKSBnIKsqr9J\nMpRkn6q6N8lHk8xK0lprF7bWvlFVr62qu7P5NhTvGMR6YSq46KKL8prXvCYPPvhg3vnOd2b+/PlZ\nt25dvvzlL+ecc87JxRdfnD333HOipwlARwM7BTkoTkEyHa1cuTJLly7NVVddlYULF+bBBx/MUUcd\nld///d/PscceO9HTA2AnTfhtKAZFgDGdrVmzJqtXr87ee++dfffdd6KnA8A4CTAAgM4mw0X4AABs\nh0l5HzCAp9x666255pprsnHjxhx11FE57rjjUrXD/9gEmFQcAQMmpZ/+9KcZGhrK61//+qxatSqr\nV6/Ou9/97rz0pS/NbbfdNtHTAxgX14ABk87PfvazLFmyJKeffno+9KEPZebMzQfrW2v5whe+kPe/\n//258cYbc9BBB03wTIHdnYvwgWnjk5/8ZK677rp89atfHXX5Bz/4wTz22GP5xCc+0XlmAM8kwIBp\n4xWveEX++I//OCeccMKoy++7774cfvjhWbNmjevBgAnlpyCBaWP16tV58YtfPObyAw44II8//njW\nrVvXcVYAgyPAgElnv/32y5133jnm8lWrVmXu3LmZPXt2x1kBDI4AAyadt7/97fnUpz6VsS5H+NSn\nPpUzzzzT6UdgyhJgwKTztre9LT/+8Y/zkY98JE8++eTTr2/atCkXXXRRvvjFL+Z973vfBM4QYHzc\niBWYdPbaa6+sWLEiZ555Zg488MCccsopmTVrVq688srsvffeufbaa3PAAQdM9DQBdpqfggQmtZUr\nVz59J/xjjjkmr3jFK5x6BCYNt6EAAOjMbSgAAKYIAQYA0JkAAwDoTIABAHQmwAAAOhNgAACdCTAA\ngM4EGABAZwIMAKAzAQYA0JkAAwDoTIABAHQmwAAAOhNgAACdCTAAgM4EGABAZwIMAKAzAQYA0JkA\nAwDoTIABAHQmwAAAOhNgAACdCTAAgM4EGABAZwIMAKAzAQYA0JkAAwDoTIABAHQmwAAAOhNgAACd\nCTAAgM4EGHTwyCOP5OMf/3gOOeSQzJ07NwceeGA+9KEP5ac//elETw0G5pFHHsl5552XRYsWZdas\nWVmwYEF+7/d+z34Oo6jW2kTP4Rmqqk22OcF4rF69OkNDQzniiCNy9tlnZ/HixVm1alUuuOCCXHLJ\nJbn66quzePHiiZ4mjMvq1auzZMmSPPjgg1m3bt3Tr8+aNStz587N9ddfbz9nWqqqtNZqh9832WJH\ngDHdvP71r88v//Iv54/+6I+es+zzn/98Pvaxj+X222/PjBkOSDN1/eqv/mquv/76bNiw4TnLqir7\n779/7rnnHvs5087OBpg/CbAL/cu//EtuvPHGfPjDHx51+W/91m9l9uzZWbFiReeZweA8tZ+PFl9J\n0lrLmjVr7OewBQEGu9ANN9yQE088MXPmzBl1eVXljW98Y775zW92nhkMzg033JCZM2dudcyjjz6a\n4eHhPhOCKUCAAQB0JsBgFzr22GNz9dVX5/HHHx91eWstX//613Pcccd1nhkMzrHHHjvm6cen7LXX\nXhkaGuozIZgCBBjsQi960Yty9NFH59xzzx11+Re+8IWsW7cuv/Zrv9Z5ZjA4T+3nY52GrKq84AUv\nsJ/DFvwUJOxiT92G4mUve1nOPvvsHHbYYU/fhuLSSy/N8uXL/Xg+U57bULC78lOQMEktXLgwN954\nY4444oiceeaZWbhwYV772tfm+c9/fm666SZ/KTEtLFy4MD/4wQ/yB3/wB1m0aFH22GOP7LPPPjn7\n7LNz++2328/hWRwBAwDYSY6AAQBMEQIMAKAzAQYA0JkAAwDoTIABAHQmwAAAOhNgsBv4/ve/n3e+\n85159atfnTPOOCPXXXdd3O4FYOK4DxhMYxs2bMgZZ5yRZcuW5YknnsjGjRtTVZk3b14OO+ywXHXV\nVfm5n/u5iZ4mwJQ1ofcBq6qTquqOqrqzqt4/yvLjq+rhqvr+yNdHBrFeYOve9773ZdmyZVm7dm02\nbtyYZPMvAH/00Udzyy235E1vetMEzxBg9zTuI2BVNSPJnUlOSPLTJDcleWtr7Y4txhyf5P+01k7Z\njs9zBAwG4OGHH85+++33jN/L92xz5szJzTffnEMPPbTjzACmj4k8ArYkyV2ttVWttfVJvpTkDaOM\n2+HJATtv+fLl2WOPPbY6Zv369bn00ks7zQiApwwiwBYluW+L5z8Zee3ZjqmqW6vq76rKP7dhF3vs\nsceyadOmrY7ZsGFD/vM//7PTjAB4ysxO6/lekgNba2ur6uQkX09ycKd1w27pkEMOSdXWDzzPmzcv\nixcv7jQjAJ4yiAC7P8mBWzzff+S1p7XWHt3i8ZVV9emq2ru19tBoH7h06dKnHw8NDWVoaGgA04Td\nyzHHHJMFCxbk0Ucf3eq4U089tdOMAKa+4eHhDA8Pj/tzBnER/vOS/CibL8J/IMl3k5zeWlu5xZiF\nrbXVI4+XJPlKa+0Xxvg8F+HDgNxwww058cQTs3bt2ucsmzNnTj7zmc/kN3/zNydgZgDTw4RdhN9a\n25jkrCTLk9ye5EuttZVV9dtV9a6RYW+uqn+uqluS/N8kp413vcC2vfKVr8zw8HCWLFmS2bNnZ/78\n+Zk7d24OPvjgXHLJJeILYIK4ESvsJlatWpX7778/++yzTw455JCJng7AtLCzR8AEGADATprQO+ED\nALD9BBgAQGcCDACgMwEGANCZAAMA6EyAAQB0JsAAADoTYAAAnQkwAIDOBBgAQGcCDACgMwEGANCZ\nAAMA6EyAAQB0JsAAADoTYAAAnQkwAIDOBBgAQGcCDACgMwEGANCZAAMA6EyAAQB0JsAAADoTYAAA\nnQkwAIDOBBgAQGcCDACgMwEGANCZAAMYsPXr1+crX/lKTjrppCxevDjHH398Lrroojz22GMTPTVg\nkqjW2kTP4Rmqqk22OQFsr0cffTSve93rsn79+rznPe/JoYcemnvvvTcXXHBB7rrrrqxYsSKLFi2a\n6GkCA1JVaa3VDr9vssWOAAOmsre//e2ZMWNGPvvZz2bGjGeeZDjvvPOybNmy/OM//mOqdvj7NTAJ\nCTCACfbAAw/k0EMPzT333JP58+c/Z/mmTZty8MEH56//+q9z9NFHT8AMgUHb2QBzDRjAgFx77bU5\n4YQTRo2vJJkxY0ZOO+20XHnllZ1nBkw2AgxgQNavX5+5c+dudcycOXOyfv36TjMCJisBBjCGDRs2\n5Ic//GFuu+22PP7449scf+SRR+baa6/Nhg0bxhyzfPnyHHnkkYOcJjAFCTCAZ9m4cWPOPffcLFy4\nMEcddVSOPfbY7Lvvvnnve9+71VtJHH744fnFX/zFXHDBBaMuv/rqq/PjH/84p5xyyq6aOjBFuAgf\nYAubNm3Km9/85lx11VVZu3btM5bNnj07L3nJS3LDDTdkzpw5o77/zjvvzNDQUM4888ycddZZ+fmf\n//k8/PDD+dznPpfzzjsvl1xySY4//vge/ytABy7CBxiAK664IsuXL39OfCXJunXr8qMf/Sif/OQn\nx3z/wQcfnBtvvDFr1qzJL/3SL2XBggXZf//9c9NNN+Waa64RX0ASR8AAnuFVr3pVrr/++q2OeeEL\nX5gHHnhgm5/15JNP5pFHHsnzn//8zJ49e1BTBCYR9wEDGIAFCxbkP/7jP7Y6ZsaMGVm7dm323HPP\nTrMCJiunIAEGYHuOVFVV9thjjw6zAaYrAQawhVNPPTWzZs3a6pgTTjjhOb9mCGBHOAUJsIVVq1Zl\n8eLFY95uYs6cOVmxYkVe+cpXdp4ZMBk5BQkwAAcddFCWLVuWvfba6xl3td9zzz0ze/bsnH/++eIL\nGDdHwABGsWbNmlx88cX52te+lo0bN+aEE07Iu9/97ixatGiipwZMIn4KEgCgM6cgAQCmCAEGANCZ\nAAOYRtauXZvzzz8/S5YsyX777ZfDDz88H//4x/PQQw9N9NSALQgwgGnioYceyqte9apcccUVOffc\nc3PzzTfnwgsvzMqVK/Pyl788d99990RPERjhInyAaeItb3lLFi1alD//8z9P1TOvCf70pz+dCy+8\nMLfccstzlgE7z09BAuzG7r333hxxxBG59957M2/evOcsb63lsMMOy1/+5V9maGio/wRhmvJTkAC7\nsW9+85s58cQTR42vZPNfEm9605tyzTXXdJ4ZMBoBBjANbNq0KTNnztzqmD322CObNm3qNCNgawQY\nwDRw1FFHZcWKFXniiSfGHLNs2bIcc8wxHWcFjEWAAUwDhxxySA4//PD82Z/92ajLv/rVr+bf/u3f\ncvLJJ3eeGTCarR+vBmDKuPjii3PcccflnnvuyXvf+9685CUvyU9+8pNccMEFueiii/KNb3wjz3ve\n8yZ6mkAcAQOYNg444IB897vfzYIFC/LqV786M2fOzOGHH56HH3443/72t3PkkUdO9BSBEW5DATBN\nbdiwYZsX5gPj4zYUADyD+ILJS4ABAHQmwAAAOhNgAACdCTAAgM4EGDBpfetb38rJJ5+cuXPnZvbs\n2Tn66KNz2WWXxU9KA1PdQAKsqk6qqjuq6s6qev8YYz5RVXdV1a1V9bJBrBeYvv70T/80J510Uv7+\n7/8+jz/+eJ544ol85zvfyRlnnJF3vetdIgyY0sZ9H7CqmpHkziQnJPlpkpuSvLW1dscWY05OclZr\n7X9U1VFJ/qK1dvQYn+c+YLCbu/nmm3P88cdn7dq1oy6fN29ePvOZz+Stb31r55kBPNNE3gdsSZK7\nWmurWmvrk3wpyRueNeYNST6fJK217ySZX1ULB7BuYBr6kz/5k6xbt27M5Y899ljOPffcjjMCGKxB\nBNiiJPdt8fwnI69tbcz9o4wBSJL8wz/8QzZt2rTVMT/84Q+zfv36TjMCGKxJeZvkpUuXPv14aGgo\nQ0NDEzYXAICnDA8PZ3h4eNyfM4hrwI5OsrS1dtLI8w8kaa21j28x5vwk17XWvjzy/I4kx7fWVo/y\nea4Bg93cqaeemksvvXSrR8EOO+yw3HbbbR1nBfBcE3kN2E1JXlxVB1XVrCRvTXL5s8ZcnuRtydPB\n9vBo8QWQJOecc05mz5495vJ58+blwx/+cMcZAQzWuAOstbYxyVlJlie5PcmXWmsrq+q3q+pdI2O+\nkeT/VdXdSS5I8jvjXS8wff3Kr/xK/vAP/zBz5859zrJ58+bl9NNPz2mnnTYBMwMYjHGfghw0pyCB\np3zrW9/Kxz72sQwPD2fTpk054ogj8oEPfCCnnHJKqnb4iD/AwO3sKUgBBgCwkybyGjAAAHaAAAMA\n6EyAAQB0JsAAADoTYAAAnQkwAIDOBBgAQGcCDACgMwEGANCZAAMA6EyAAQB0JsAAADoTYAAAnQkw\nAIDOBBgAQGcCDACgMwEGANCZAAMA6EyAAQB0JsAAADoTYAAAnQkwAIDOBBgAQGcCDACgMwEGANCZ\nAAMA6EyAAQB0JsAAADoTYAAAnQkwAIDOBBgAQGcCDACgMwEGANCZAAMA6EyAAQB0JsAAADoTYAAA\nnQkwAIDOBBgAQGcCDACgMwEGANCZAAMA6EyAAQB0JsAAADoTYAAAnQkwAIDOBBgAQGcCDACgMwEG\nANCZAAMA6EyAAQB0JsAAADoTYAAAnQkwAIDOBBgAQGcCDACgMwEGANCZAAMA6EyAAQB0JsAAADoT\nYAAAnQkwAIDOBBgAQGcCDACgs5njeXNVvSDJl5MclOSeJKe21h4ZZdw9SR5JsinJ+tbakvGsFwBg\nKhvvEbAPJFnRWjskybVJPjjGuE1JhlprR4gvAGB3N94Ae0OSvxp5/FdJ3jjGuBrAugAApoXxRtG+\nrbXVSdJa+9ck+44xriW5uqpuqqp3jnOdAABT2javAauqq5Ms3PKlbA6qj4wyvI3xMce21h6oqv+S\nzSG2srV2/VjrXLp06dOPh4aGMjQ0tK1pAgDscsPDwxkeHh7351RrYzXTdry5amU2X9u1uqpemOS6\n1tovbeM9H03ys9ban42xvI1nTgAAvVRVWmu1o+8b7ynIy5OcOfL47Ukue/aAqppbVXuNPJ6X5DVJ\n/nmc6wUAmLLGewRs7yRfSXJAklXZfBuKh6tqvyQXtdZeV1W/mORr2Xx6cmaSL7bW/ngrn+kIGAAw\nJezsEbBxBdiuIMAAgKliok5BAgCwgwQYAEBnAgwAoDMBBgDQmQADAOhMgAEAdCbAAAA6E2AAAJ0J\nMACAzgQYAEBnAgwAoDMBBgDQmQADAOhMgAEAdCbAAAA6E2AAAJ0JMACAzgQYAEBnAgwAoDMBBgDQ\nmQADAOhMgAEAdCbAAAA6E2AAAJ0JMACAzgQYAEBnAgwAoDMBBgDQmQADAOhMgAEAdCbAAAA6E2AA\nAJ0JMACAzgQYAEBnAgwAoDMBBgDQmQADAOhMgAEAdCbAAAA6E2AAAJ0JMACAzgQYAEBnAgwAoDMB\nBgDQmQADAOhMgAEAdCbAAAA6E2AAAJ0JMACAzgQYAEBnAgwAoDMBBgDQmQADAOhMgAEAdCbAAAA6\nE2AAAJ0JMACAzgQYAEBnAgwAoDMBBgDQmQADAOhMgAEAdCbAAAA6E2AAAJ0JMACAzgQYAEBn4wqw\nqnpzVf1zVW2sqpdvZdxJVXVHVd1ZVe8fzzoZvOHh4Ymewm7HNu/PNu/PNu/PNp86xnsE7LYk/zPJ\nP4w1oKpmJPlUkl9PsjjJ6VX1knGulwHyB7Y/27w/27w/27w/23zqmDmeN7fWfpQkVVVbGbYkyV2t\ntVUjY7+U5A1J7hjPugEApqoe14AtSnLfFs9/MvIaAMBuqVprWx9QdXWShVu+lKQl+XBr7YqRMdcl\n+T+tte+P8v43Jfn11tq7Rp7/ryRLWmvvGWN9W58QAMAk0lrb2pnAUW3zFGRr7cSdm87T7k9y4BbP\n9x95baz17fD/BADAVDLIU5BjhdNNSV5cVQdV1awkb01y+QDXCwAwpYz3NhRvrKr7khydZFlVXTny\n+n5VtSxJWmsbk5yVZHmS25N8qbW2cnzTBgCYurZ5DRgAAIM1oXfCdyPX/qrqBVW1vKp+VFVXVdX8\nMcbdU1X/VFW3VNV3e89zOtie/baqPlFVd1XVrVX1st5znG62tc2r6viqeriqvj/y9ZGJmOd0UlWf\nrarVVfWDrYyxnw/Qtra5/Xywqmr/qrq2qm6vqtuqaqwfItyh/XyifxWRG7n294EkK1prhyS5NskH\nxxi3KclQa+2I1tqSbrObJrZnv62qk5P819baf0vy20nO7z7RaWQHvld8s7X28pGvP+o6yenpc9m8\nzUdlP98ltrrNR9jPB2dDkve11hYnOSbJ7w7i+/mEBlhr7Uettbsy9gX8yRY3cm2trU/y1I1c2Tlv\nSPJXI4//KskbxxhXmfhAn8q2Z799Q5LPJ0lr7TtJ5lfVwrCztvd7hZ+0HqDW2vVJ1mxliP18wLZj\nmyf284Fprf1ra+3WkcePJlmZ597PdIf386nwF6wbuQ7Wvq211cnmnSrJvmOMa0murqqbquqd3WY3\nfWzPfvvsMfePMobtt73fK44ZOUXwd1V1aJ+p7dbs5xPDfr4LVNUvJHlZku88a9EO7+fj+lVE22N7\nbuTKYG1lm492HcBYP4VxbGvtgar6L9kcYitH/tUFU9n3khzYWls7csrg60kOnuA5waDZz3eBqtor\nySVJzh45EjYuuzzAet/Ila1v85ELNxe21lZX1QuTPDjGZzww8t9/q6qvZfPpHQG2/bZnv70/yQHb\nGMP22+Y23/KbZmvtyqr6dFXt3Vp7qNMcd0f2887s54NXVTOzOb6+0Fq7bJQhO7yfT6ZTkG7k2sfl\nSc4cefz2JM/Zkapq7kjpp6rmJXlNkn/uNcFpYnv228uTvC1JquroJA8/dXqYnbLNbb7lNRlVtSSb\nb8XjL6Xxq4z9Pdx+vmuMuc3t57vExUl+2Fr7izGW7/B+vsuPgG1NVb0xySeTLMjmG7ne2lo7uar2\nS3JRa+11rbWNVfXUjVxnJPmsG7mOy8eTfKWq/neSVUlOTTbfPDcj2zybT19+beT3cs5M8sXW2vKJ\nmvBUNNZ+W1W/vXlxu7C19o2qem1V3Z3ksSTvmMg5T3Xbs82TvLmq3p1kfZLHk5w2cTOeHqrqb5IM\nJdmnqu5N8tEks2I/32W2tc1jPx+oqjo2yRlJbquqW7L50p0PJTko49jP3YgVAKCzyXQKEgBgtyDA\nAAA6E2AAAJ0JMACAzgQYAEBnAgwAoDMBBgDQ2f8HUNzUEBm6WX0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108f42850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "np.random.seed(1)\n",
    "# N = 4 # number of points per class\n",
    "# D = 2 # dimensionality\n",
    "# K = 2 # number of classes\n",
    "# X = np.zeros((N*K,D))\n",
    "# y = np.zeros(N*K, dtype='uint8')\n",
    "# for j in xrange(K):\n",
    "#   ix = range(N*j,N*(j+1))\n",
    "#   r = np.linspace(0.0,1,N) # radius\n",
    "#   t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "#   X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "#   y[ix] = j\n",
    "\n",
    "\n",
    "# Linearly separable case for the Logistic regression\n",
    "# X = np.array([[5.4,98],\n",
    "#     [5.8,120],\n",
    "#     [6.2,168],\n",
    "#     [5.4,200],\n",
    "#     [5,210],\n",
    "#     [5,168]])\n",
    "\n",
    "# Linearly non-separable case for the NN\n",
    "X = np.array([[5.5,98],\n",
    "    [5.2,155],\n",
    "    [5.7,120],\n",
    "    [6.2,168],\n",
    "    [5.5,200],\n",
    "    [5,210],\n",
    "    [5,168],\n",
    "    [6.0,110],\n",
    "    [5.8,125]])\n",
    "\n",
    "\n",
    "\n",
    "X1 = map(lambda x: (x-np.min(X[:,0]))/(np.max(X[:,0])-np.min(X[:,0])), X[:,0])\n",
    "X2 = map(lambda x: (x-np.min(X[:,1]))/(np.max(X[:,1])-np.min(X[:,1])), X[:,1])\n",
    "X = np.array(zip(X1,X2))\n",
    "\n",
    "y= [0,0,0,0,1,1,1,1,1]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=80)\n",
    "plt.xlim([-1,2])\n",
    "plt.ylim([-1,2])\n",
    "#fig.savefig('spiral_raw.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.693333\n",
      "iteration 10: loss 0.633127\n",
      "iteration 20: loss 0.621481\n",
      "iteration 30: loss 0.618128\n",
      "iteration 40: loss 0.616887\n",
      "iteration 50: loss 0.616306\n",
      "iteration 60: loss 0.615981\n",
      "iteration 70: loss 0.615781\n",
      "iteration 80: loss 0.615652\n",
      "iteration 90: loss 0.615566\n",
      "iteration 100: loss 0.615509\n",
      "iteration 110: loss 0.615471\n",
      "iteration 120: loss 0.615445\n",
      "iteration 130: loss 0.615428\n",
      "iteration 140: loss 0.615417\n",
      "iteration 150: loss 0.615409\n",
      "iteration 160: loss 0.615404\n",
      "iteration 170: loss 0.615400\n",
      "iteration 180: loss 0.615398\n",
      "iteration 190: loss 0.615396\n",
      "iteration 200: loss 0.615395\n",
      "iteration 210: loss 0.615394\n",
      "iteration 220: loss 0.615394\n",
      "iteration 230: loss 0.615394\n",
      "iteration 240: loss 0.615393\n",
      "iteration 250: loss 0.615393\n",
      "iteration 260: loss 0.615393\n",
      "iteration 270: loss 0.615393\n",
      "iteration 280: loss 0.615393\n",
      "iteration 290: loss 0.615393\n",
      "iteration 300: loss 0.615393\n",
      "iteration 310: loss 0.615393\n",
      "iteration 320: loss 0.615393\n",
      "iteration 330: loss 0.615393\n",
      "iteration 340: loss 0.615393\n",
      "iteration 350: loss 0.615393\n",
      "iteration 360: loss 0.615393\n",
      "iteration 370: loss 0.615393\n",
      "iteration 380: loss 0.615393\n",
      "iteration 390: loss 0.615393\n",
      "iteration 400: loss 0.615393\n",
      "iteration 410: loss 0.615393\n",
      "iteration 420: loss 0.615393\n",
      "iteration 430: loss 0.615393\n",
      "iteration 440: loss 0.615393\n",
      "iteration 450: loss 0.615393\n",
      "iteration 460: loss 0.615393\n",
      "iteration 470: loss 0.615393\n",
      "iteration 480: loss 0.615393\n",
      "iteration 490: loss 0.615393\n",
      "iteration 500: loss 0.615393\n",
      "iteration 510: loss 0.615393\n",
      "iteration 520: loss 0.615393\n",
      "iteration 530: loss 0.615393\n",
      "iteration 540: loss 0.615393\n",
      "iteration 550: loss 0.615393\n",
      "iteration 560: loss 0.615393\n",
      "iteration 570: loss 0.615393\n",
      "iteration 580: loss 0.615393\n",
      "iteration 590: loss 0.615393\n",
      "iteration 600: loss 0.615393\n",
      "iteration 610: loss 0.615393\n",
      "iteration 620: loss 0.615393\n",
      "iteration 630: loss 0.615393\n",
      "iteration 640: loss 0.615393\n",
      "iteration 650: loss 0.615393\n",
      "iteration 660: loss 0.615393\n",
      "iteration 670: loss 0.615393\n",
      "iteration 680: loss 0.615393\n",
      "iteration 690: loss 0.615393\n",
      "iteration 700: loss 0.615393\n",
      "iteration 710: loss 0.615393\n",
      "iteration 720: loss 0.615393\n",
      "iteration 730: loss 0.615393\n",
      "iteration 740: loss 0.615393\n",
      "iteration 750: loss 0.615393\n",
      "iteration 760: loss 0.615393\n",
      "iteration 770: loss 0.615393\n",
      "iteration 780: loss 0.615393\n",
      "iteration 790: loss 0.615393\n",
      "iteration 800: loss 0.615393\n",
      "iteration 810: loss 0.615393\n",
      "iteration 820: loss 0.615393\n",
      "iteration 830: loss 0.615393\n",
      "iteration 840: loss 0.615393\n",
      "iteration 850: loss 0.615393\n",
      "iteration 860: loss 0.615393\n",
      "iteration 870: loss 0.615393\n",
      "iteration 880: loss 0.615393\n",
      "iteration 890: loss 0.615393\n",
      "iteration 900: loss 0.615393\n",
      "iteration 910: loss 0.615393\n",
      "iteration 920: loss 0.615393\n",
      "iteration 930: loss 0.615393\n",
      "iteration 940: loss 0.615393\n",
      "iteration 950: loss 0.615393\n",
      "iteration 960: loss 0.615393\n",
      "iteration 970: loss 0.615393\n",
      "iteration 980: loss 0.615393\n",
      "iteration 990: loss 0.615393\n",
      "iteration 1000: loss 0.615393\n",
      "iteration 1010: loss 0.615393\n",
      "iteration 1020: loss 0.615393\n",
      "iteration 1030: loss 0.615393\n",
      "iteration 1040: loss 0.615393\n",
      "iteration 1050: loss 0.615393\n",
      "iteration 1060: loss 0.615393\n",
      "iteration 1070: loss 0.615393\n",
      "iteration 1080: loss 0.615393\n",
      "iteration 1090: loss 0.615393\n",
      "iteration 1100: loss 0.615393\n",
      "iteration 1110: loss 0.615393\n",
      "iteration 1120: loss 0.615393\n",
      "iteration 1130: loss 0.615393\n",
      "iteration 1140: loss 0.615393\n",
      "iteration 1150: loss 0.615393\n",
      "iteration 1160: loss 0.615393\n",
      "iteration 1170: loss 0.615393\n",
      "iteration 1180: loss 0.615393\n",
      "iteration 1190: loss 0.615393\n",
      "iteration 1200: loss 0.615393\n",
      "iteration 1210: loss 0.615393\n",
      "iteration 1220: loss 0.615393\n",
      "iteration 1230: loss 0.615393\n",
      "iteration 1240: loss 0.615393\n",
      "iteration 1250: loss 0.615393\n",
      "iteration 1260: loss 0.615393\n",
      "iteration 1270: loss 0.615393\n",
      "iteration 1280: loss 0.615393\n",
      "iteration 1290: loss 0.615393\n",
      "iteration 1300: loss 0.615393\n",
      "iteration 1310: loss 0.615393\n",
      "iteration 1320: loss 0.615393\n",
      "iteration 1330: loss 0.615393\n",
      "iteration 1340: loss 0.615393\n",
      "iteration 1350: loss 0.615393\n",
      "iteration 1360: loss 0.615393\n",
      "iteration 1370: loss 0.615393\n",
      "iteration 1380: loss 0.615393\n",
      "iteration 1390: loss 0.615393\n",
      "iteration 1400: loss 0.615393\n",
      "iteration 1410: loss 0.615393\n",
      "iteration 1420: loss 0.615393\n",
      "iteration 1430: loss 0.615393\n",
      "iteration 1440: loss 0.615393\n",
      "iteration 1450: loss 0.615393\n",
      "iteration 1460: loss 0.615393\n",
      "iteration 1470: loss 0.615393\n",
      "iteration 1480: loss 0.615393\n",
      "iteration 1490: loss 0.615393\n",
      "iteration 1500: loss 0.615393\n",
      "iteration 1510: loss 0.615393\n",
      "iteration 1520: loss 0.615393\n",
      "iteration 1530: loss 0.615393\n",
      "iteration 1540: loss 0.615393\n",
      "iteration 1550: loss 0.615393\n",
      "iteration 1560: loss 0.615393\n",
      "iteration 1570: loss 0.615393\n",
      "iteration 1580: loss 0.615393\n",
      "iteration 1590: loss 0.615393\n",
      "iteration 1600: loss 0.615393\n",
      "iteration 1610: loss 0.615393\n",
      "iteration 1620: loss 0.615393\n",
      "iteration 1630: loss 0.615393\n",
      "iteration 1640: loss 0.615393\n",
      "iteration 1650: loss 0.615393\n",
      "iteration 1660: loss 0.615393\n",
      "iteration 1670: loss 0.615393\n",
      "iteration 1680: loss 0.615393\n",
      "iteration 1690: loss 0.615393\n",
      "iteration 1700: loss 0.615393\n",
      "iteration 1710: loss 0.615393\n",
      "iteration 1720: loss 0.615393\n",
      "iteration 1730: loss 0.615393\n",
      "iteration 1740: loss 0.615393\n",
      "iteration 1750: loss 0.615393\n",
      "iteration 1760: loss 0.615393\n",
      "iteration 1770: loss 0.615393\n",
      "iteration 1780: loss 0.615393\n",
      "iteration 1790: loss 0.615393\n",
      "iteration 1800: loss 0.615393\n",
      "iteration 1810: loss 0.615393\n",
      "iteration 1820: loss 0.615393\n",
      "iteration 1830: loss 0.615393\n",
      "iteration 1840: loss 0.615393\n",
      "iteration 1850: loss 0.615393\n",
      "iteration 1860: loss 0.615393\n",
      "iteration 1870: loss 0.615393\n",
      "iteration 1880: loss 0.615393\n",
      "iteration 1890: loss 0.615393\n",
      "iteration 1900: loss 0.615393\n",
      "iteration 1910: loss 0.615393\n",
      "iteration 1920: loss 0.615393\n",
      "iteration 1930: loss 0.615393\n",
      "iteration 1940: loss 0.615393\n",
      "iteration 1950: loss 0.615393\n",
      "iteration 1960: loss 0.615393\n",
      "iteration 1970: loss 0.615393\n",
      "iteration 1980: loss 0.615393\n",
      "iteration 1990: loss 0.615393\n",
      "iteration 2000: loss 0.615393\n",
      "iteration 2010: loss 0.615393\n",
      "iteration 2020: loss 0.615393\n",
      "iteration 2030: loss 0.615393\n",
      "iteration 2040: loss 0.615393\n",
      "iteration 2050: loss 0.615393\n",
      "iteration 2060: loss 0.615393\n",
      "iteration 2070: loss 0.615393\n",
      "iteration 2080: loss 0.615393\n",
      "iteration 2090: loss 0.615393\n",
      "iteration 2100: loss 0.615393\n",
      "iteration 2110: loss 0.615393\n",
      "iteration 2120: loss 0.615393\n",
      "iteration 2130: loss 0.615393\n",
      "iteration 2140: loss 0.615393\n",
      "iteration 2150: loss 0.615393\n",
      "iteration 2160: loss 0.615393\n",
      "iteration 2170: loss 0.615393\n",
      "iteration 2180: loss 0.615393\n",
      "iteration 2190: loss 0.615393\n",
      "iteration 2200: loss 0.615393\n",
      "iteration 2210: loss 0.615393\n",
      "iteration 2220: loss 0.615393\n",
      "iteration 2230: loss 0.615393\n",
      "iteration 2240: loss 0.615393\n",
      "iteration 2250: loss 0.615393\n",
      "iteration 2260: loss 0.615393\n",
      "iteration 2270: loss 0.615393\n",
      "iteration 2280: loss 0.615393\n",
      "iteration 2290: loss 0.615393\n",
      "iteration 2300: loss 0.615393\n",
      "iteration 2310: loss 0.615393\n",
      "iteration 2320: loss 0.615393\n",
      "iteration 2330: loss 0.615393\n",
      "iteration 2340: loss 0.615393\n",
      "iteration 2350: loss 0.615393\n",
      "iteration 2360: loss 0.615393\n",
      "iteration 2370: loss 0.615393\n",
      "iteration 2380: loss 0.615393\n",
      "iteration 2390: loss 0.615393\n",
      "iteration 2400: loss 0.615393\n",
      "iteration 2410: loss 0.615393\n",
      "iteration 2420: loss 0.615393\n",
      "iteration 2430: loss 0.615393\n",
      "iteration 2440: loss 0.615393\n",
      "iteration 2450: loss 0.615393\n",
      "iteration 2460: loss 0.615393\n",
      "iteration 2470: loss 0.615393\n",
      "iteration 2480: loss 0.615393\n",
      "iteration 2490: loss 0.615393\n",
      "iteration 2500: loss 0.615393\n",
      "iteration 2510: loss 0.615393\n",
      "iteration 2520: loss 0.615393\n",
      "iteration 2530: loss 0.615393\n",
      "iteration 2540: loss 0.615393\n",
      "iteration 2550: loss 0.615393\n",
      "iteration 2560: loss 0.615393\n",
      "iteration 2570: loss 0.615393\n",
      "iteration 2580: loss 0.615393\n",
      "iteration 2590: loss 0.615393\n",
      "iteration 2600: loss 0.615393\n",
      "iteration 2610: loss 0.615393\n",
      "iteration 2620: loss 0.615393\n",
      "iteration 2630: loss 0.615393\n",
      "iteration 2640: loss 0.615393\n",
      "iteration 2650: loss 0.615393\n",
      "iteration 2660: loss 0.615393\n",
      "iteration 2670: loss 0.615393\n",
      "iteration 2680: loss 0.615393\n",
      "iteration 2690: loss 0.615393\n",
      "iteration 2700: loss 0.615393\n",
      "iteration 2710: loss 0.615393\n",
      "iteration 2720: loss 0.615393\n",
      "iteration 2730: loss 0.615393\n",
      "iteration 2740: loss 0.615393\n",
      "iteration 2750: loss 0.615393\n",
      "iteration 2760: loss 0.615393\n",
      "iteration 2770: loss 0.615393\n",
      "iteration 2780: loss 0.615393\n",
      "iteration 2790: loss 0.615393\n",
      "iteration 2800: loss 0.615393\n",
      "iteration 2810: loss 0.615393\n",
      "iteration 2820: loss 0.615393\n",
      "iteration 2830: loss 0.615393\n",
      "iteration 2840: loss 0.615393\n",
      "iteration 2850: loss 0.615393\n",
      "iteration 2860: loss 0.615393\n",
      "iteration 2870: loss 0.615393\n",
      "iteration 2880: loss 0.615393\n",
      "iteration 2890: loss 0.615393\n",
      "iteration 2900: loss 0.615393\n",
      "iteration 2910: loss 0.615393\n",
      "iteration 2920: loss 0.615393\n",
      "iteration 2930: loss 0.615393\n",
      "iteration 2940: loss 0.615393\n",
      "iteration 2950: loss 0.615393\n",
      "iteration 2960: loss 0.615393\n",
      "iteration 2970: loss 0.615393\n",
      "iteration 2980: loss 0.615393\n",
      "iteration 2990: loss 0.615393\n",
      "iteration 3000: loss 0.615393\n",
      "iteration 3010: loss 0.615393\n",
      "iteration 3020: loss 0.615393\n",
      "iteration 3030: loss 0.615393\n",
      "iteration 3040: loss 0.615393\n",
      "iteration 3050: loss 0.615393\n",
      "iteration 3060: loss 0.615393\n",
      "iteration 3070: loss 0.615393\n",
      "iteration 3080: loss 0.615393\n",
      "iteration 3090: loss 0.615393\n",
      "iteration 3100: loss 0.615393\n",
      "iteration 3110: loss 0.615393\n",
      "iteration 3120: loss 0.615393\n",
      "iteration 3130: loss 0.615393\n",
      "iteration 3140: loss 0.615393\n",
      "iteration 3150: loss 0.615393\n",
      "iteration 3160: loss 0.615393\n",
      "iteration 3170: loss 0.615393\n",
      "iteration 3180: loss 0.615393\n",
      "iteration 3190: loss 0.615393\n",
      "iteration 3200: loss 0.615393\n",
      "iteration 3210: loss 0.615393\n",
      "iteration 3220: loss 0.615393\n",
      "iteration 3230: loss 0.615393\n",
      "iteration 3240: loss 0.615393\n",
      "iteration 3250: loss 0.615393\n",
      "iteration 3260: loss 0.615393\n",
      "iteration 3270: loss 0.615393\n",
      "iteration 3280: loss 0.615393\n",
      "iteration 3290: loss 0.615393\n",
      "iteration 3300: loss 0.615393\n",
      "iteration 3310: loss 0.615393\n",
      "iteration 3320: loss 0.615393\n",
      "iteration 3330: loss 0.615393\n",
      "iteration 3340: loss 0.615393\n",
      "iteration 3350: loss 0.615393\n",
      "iteration 3360: loss 0.615393\n",
      "iteration 3370: loss 0.615393\n",
      "iteration 3380: loss 0.615393\n",
      "iteration 3390: loss 0.615393\n",
      "iteration 3400: loss 0.615393\n",
      "iteration 3410: loss 0.615393\n",
      "iteration 3420: loss 0.615393\n",
      "iteration 3430: loss 0.615393\n",
      "iteration 3440: loss 0.615393\n",
      "iteration 3450: loss 0.615393\n",
      "iteration 3460: loss 0.615393\n",
      "iteration 3470: loss 0.615393\n",
      "iteration 3480: loss 0.615393\n",
      "iteration 3490: loss 0.615393\n",
      "iteration 3500: loss 0.615393\n",
      "iteration 3510: loss 0.615393\n",
      "iteration 3520: loss 0.615393\n",
      "iteration 3530: loss 0.615393\n",
      "iteration 3540: loss 0.615393\n",
      "iteration 3550: loss 0.615393\n",
      "iteration 3560: loss 0.615393\n",
      "iteration 3570: loss 0.615393\n",
      "iteration 3580: loss 0.615393\n",
      "iteration 3590: loss 0.615393\n",
      "iteration 3600: loss 0.615393\n",
      "iteration 3610: loss 0.615393\n",
      "iteration 3620: loss 0.615393\n",
      "iteration 3630: loss 0.615393\n",
      "iteration 3640: loss 0.615393\n",
      "iteration 3650: loss 0.615393\n",
      "iteration 3660: loss 0.615393\n",
      "iteration 3670: loss 0.615393\n",
      "iteration 3680: loss 0.615393\n",
      "iteration 3690: loss 0.615393\n",
      "iteration 3700: loss 0.615393\n",
      "iteration 3710: loss 0.615393\n",
      "iteration 3720: loss 0.615393\n",
      "iteration 3730: loss 0.615393\n",
      "iteration 3740: loss 0.615393\n",
      "iteration 3750: loss 0.615393\n",
      "iteration 3760: loss 0.615393\n",
      "iteration 3770: loss 0.615393\n",
      "iteration 3780: loss 0.615393\n",
      "iteration 3790: loss 0.615393\n",
      "iteration 3800: loss 0.615393\n",
      "iteration 3810: loss 0.615393\n",
      "iteration 3820: loss 0.615393\n",
      "iteration 3830: loss 0.615393\n",
      "iteration 3840: loss 0.615393\n",
      "iteration 3850: loss 0.615393\n",
      "iteration 3860: loss 0.615393\n",
      "iteration 3870: loss 0.615393\n",
      "iteration 3880: loss 0.615393\n",
      "iteration 3890: loss 0.615393\n",
      "iteration 3900: loss 0.615393\n",
      "iteration 3910: loss 0.615393\n",
      "iteration 3920: loss 0.615393\n",
      "iteration 3930: loss 0.615393\n",
      "iteration 3940: loss 0.615393\n",
      "iteration 3950: loss 0.615393\n",
      "iteration 3960: loss 0.615393\n",
      "iteration 3970: loss 0.615393\n",
      "iteration 3980: loss 0.615393\n",
      "iteration 3990: loss 0.615393\n",
      "iteration 4000: loss 0.615393\n",
      "iteration 4010: loss 0.615393\n",
      "iteration 4020: loss 0.615393\n",
      "iteration 4030: loss 0.615393\n",
      "iteration 4040: loss 0.615393\n",
      "iteration 4050: loss 0.615393\n",
      "iteration 4060: loss 0.615393\n",
      "iteration 4070: loss 0.615393\n",
      "iteration 4080: loss 0.615393\n",
      "iteration 4090: loss 0.615393\n",
      "iteration 4100: loss 0.615393\n",
      "iteration 4110: loss 0.615393\n",
      "iteration 4120: loss 0.615393\n",
      "iteration 4130: loss 0.615393\n",
      "iteration 4140: loss 0.615393\n",
      "iteration 4150: loss 0.615393\n",
      "iteration 4160: loss 0.615393\n",
      "iteration 4170: loss 0.615393\n",
      "iteration 4180: loss 0.615393\n",
      "iteration 4190: loss 0.615393\n",
      "iteration 4200: loss 0.615393\n",
      "iteration 4210: loss 0.615393\n",
      "iteration 4220: loss 0.615393\n",
      "iteration 4230: loss 0.615393\n",
      "iteration 4240: loss 0.615393\n",
      "iteration 4250: loss 0.615393\n",
      "iteration 4260: loss 0.615393\n",
      "iteration 4270: loss 0.615393\n",
      "iteration 4280: loss 0.615393\n",
      "iteration 4290: loss 0.615393\n",
      "iteration 4300: loss 0.615393\n",
      "iteration 4310: loss 0.615393\n",
      "iteration 4320: loss 0.615393\n",
      "iteration 4330: loss 0.615393\n",
      "iteration 4340: loss 0.615393\n",
      "iteration 4350: loss 0.615393\n",
      "iteration 4360: loss 0.615393\n",
      "iteration 4370: loss 0.615393\n",
      "iteration 4380: loss 0.615393\n",
      "iteration 4390: loss 0.615393\n",
      "iteration 4400: loss 0.615393\n",
      "iteration 4410: loss 0.615393\n",
      "iteration 4420: loss 0.615393\n",
      "iteration 4430: loss 0.615393\n",
      "iteration 4440: loss 0.615393\n",
      "iteration 4450: loss 0.615393\n",
      "iteration 4460: loss 0.615393\n",
      "iteration 4470: loss 0.615393\n",
      "iteration 4480: loss 0.615393\n",
      "iteration 4490: loss 0.615393\n",
      "iteration 4500: loss 0.615393\n",
      "iteration 4510: loss 0.615393\n",
      "iteration 4520: loss 0.615393\n",
      "iteration 4530: loss 0.615393\n",
      "iteration 4540: loss 0.615393\n",
      "iteration 4550: loss 0.615393\n",
      "iteration 4560: loss 0.615393\n",
      "iteration 4570: loss 0.615393\n",
      "iteration 4580: loss 0.615393\n",
      "iteration 4590: loss 0.615393\n",
      "iteration 4600: loss 0.615393\n",
      "iteration 4610: loss 0.615393\n",
      "iteration 4620: loss 0.615393\n",
      "iteration 4630: loss 0.615393\n",
      "iteration 4640: loss 0.615393\n",
      "iteration 4650: loss 0.615393\n",
      "iteration 4660: loss 0.615393\n",
      "iteration 4670: loss 0.615393\n",
      "iteration 4680: loss 0.615393\n",
      "iteration 4690: loss 0.615393\n",
      "iteration 4700: loss 0.615393\n",
      "iteration 4710: loss 0.615393\n",
      "iteration 4720: loss 0.615393\n",
      "iteration 4730: loss 0.615393\n",
      "iteration 4740: loss 0.615393\n",
      "iteration 4750: loss 0.615393\n",
      "iteration 4760: loss 0.615393\n",
      "iteration 4770: loss 0.615393\n",
      "iteration 4780: loss 0.615393\n",
      "iteration 4790: loss 0.615393\n",
      "iteration 4800: loss 0.615393\n",
      "iteration 4810: loss 0.615393\n",
      "iteration 4820: loss 0.615393\n",
      "iteration 4830: loss 0.615393\n",
      "iteration 4840: loss 0.615393\n",
      "iteration 4850: loss 0.615393\n",
      "iteration 4860: loss 0.615393\n",
      "iteration 4870: loss 0.615393\n",
      "iteration 4880: loss 0.615393\n",
      "iteration 4890: loss 0.615393\n",
      "iteration 4900: loss 0.615393\n",
      "iteration 4910: loss 0.615393\n",
      "iteration 4920: loss 0.615393\n",
      "iteration 4930: loss 0.615393\n",
      "iteration 4940: loss 0.615393\n",
      "iteration 4950: loss 0.615393\n",
      "iteration 4960: loss 0.615393\n",
      "iteration 4970: loss 0.615393\n",
      "iteration 4980: loss 0.615393\n",
      "iteration 4990: loss 0.615393\n",
      "iteration 5000: loss 0.615393\n",
      "iteration 5010: loss 0.615393\n",
      "iteration 5020: loss 0.615393\n",
      "iteration 5030: loss 0.615393\n",
      "iteration 5040: loss 0.615393\n",
      "iteration 5050: loss 0.615393\n",
      "iteration 5060: loss 0.615393\n",
      "iteration 5070: loss 0.615393\n",
      "iteration 5080: loss 0.615393\n",
      "iteration 5090: loss 0.615393\n",
      "iteration 5100: loss 0.615393\n",
      "iteration 5110: loss 0.615393\n",
      "iteration 5120: loss 0.615393\n",
      "iteration 5130: loss 0.615393\n",
      "iteration 5140: loss 0.615393\n",
      "iteration 5150: loss 0.615393\n",
      "iteration 5160: loss 0.615393\n",
      "iteration 5170: loss 0.615393\n",
      "iteration 5180: loss 0.615393\n",
      "iteration 5190: loss 0.615393\n",
      "iteration 5200: loss 0.615393\n",
      "iteration 5210: loss 0.615393\n",
      "iteration 5220: loss 0.615393\n",
      "iteration 5230: loss 0.615393\n",
      "iteration 5240: loss 0.615393\n",
      "iteration 5250: loss 0.615393\n",
      "iteration 5260: loss 0.615393\n",
      "iteration 5270: loss 0.615393\n",
      "iteration 5280: loss 0.615393\n",
      "iteration 5290: loss 0.615393\n",
      "iteration 5300: loss 0.615393\n",
      "iteration 5310: loss 0.615393\n",
      "iteration 5320: loss 0.615393\n",
      "iteration 5330: loss 0.615393\n",
      "iteration 5340: loss 0.615393\n",
      "iteration 5350: loss 0.615393\n",
      "iteration 5360: loss 0.615393\n",
      "iteration 5370: loss 0.615393\n",
      "iteration 5380: loss 0.615393\n",
      "iteration 5390: loss 0.615393\n",
      "iteration 5400: loss 0.615393\n",
      "iteration 5410: loss 0.615393\n",
      "iteration 5420: loss 0.615393\n",
      "iteration 5430: loss 0.615393\n",
      "iteration 5440: loss 0.615393\n",
      "iteration 5450: loss 0.615393\n",
      "iteration 5460: loss 0.615393\n",
      "iteration 5470: loss 0.615393\n",
      "iteration 5480: loss 0.615393\n",
      "iteration 5490: loss 0.615393\n",
      "iteration 5500: loss 0.615393\n",
      "iteration 5510: loss 0.615393\n",
      "iteration 5520: loss 0.615393\n",
      "iteration 5530: loss 0.615393\n",
      "iteration 5540: loss 0.615393\n",
      "iteration 5550: loss 0.615393\n",
      "iteration 5560: loss 0.615393\n",
      "iteration 5570: loss 0.615393\n",
      "iteration 5580: loss 0.615393\n",
      "iteration 5590: loss 0.615393\n",
      "iteration 5600: loss 0.615393\n",
      "iteration 5610: loss 0.615393\n",
      "iteration 5620: loss 0.615393\n",
      "iteration 5630: loss 0.615393\n",
      "iteration 5640: loss 0.615393\n",
      "iteration 5650: loss 0.615393\n",
      "iteration 5660: loss 0.615393\n",
      "iteration 5670: loss 0.615393\n",
      "iteration 5680: loss 0.615393\n",
      "iteration 5690: loss 0.615393\n",
      "iteration 5700: loss 0.615393\n",
      "iteration 5710: loss 0.615393\n",
      "iteration 5720: loss 0.615393\n",
      "iteration 5730: loss 0.615393\n",
      "iteration 5740: loss 0.615393\n",
      "iteration 5750: loss 0.615393\n",
      "iteration 5760: loss 0.615393\n",
      "iteration 5770: loss 0.615393\n",
      "iteration 5780: loss 0.615393\n",
      "iteration 5790: loss 0.615393\n",
      "iteration 5800: loss 0.615393\n",
      "iteration 5810: loss 0.615393\n",
      "iteration 5820: loss 0.615393\n",
      "iteration 5830: loss 0.615393\n",
      "iteration 5840: loss 0.615393\n",
      "iteration 5850: loss 0.615393\n",
      "iteration 5860: loss 0.615393\n",
      "iteration 5870: loss 0.615393\n",
      "iteration 5880: loss 0.615393\n",
      "iteration 5890: loss 0.615393\n",
      "iteration 5900: loss 0.615393\n",
      "iteration 5910: loss 0.615393\n",
      "iteration 5920: loss 0.615393\n",
      "iteration 5930: loss 0.615393\n",
      "iteration 5940: loss 0.615393\n",
      "iteration 5950: loss 0.615393\n",
      "iteration 5960: loss 0.615393\n",
      "iteration 5970: loss 0.615393\n",
      "iteration 5980: loss 0.615393\n",
      "iteration 5990: loss 0.615393\n",
      "iteration 6000: loss 0.615393\n",
      "iteration 6010: loss 0.615393\n",
      "iteration 6020: loss 0.615393\n",
      "iteration 6030: loss 0.615393\n",
      "iteration 6040: loss 0.615393\n",
      "iteration 6050: loss 0.615393\n",
      "iteration 6060: loss 0.615393\n",
      "iteration 6070: loss 0.615393\n",
      "iteration 6080: loss 0.615393\n",
      "iteration 6090: loss 0.615393\n",
      "iteration 6100: loss 0.615393\n",
      "iteration 6110: loss 0.615393\n",
      "iteration 6120: loss 0.615393\n",
      "iteration 6130: loss 0.615393\n",
      "iteration 6140: loss 0.615393\n",
      "iteration 6150: loss 0.615393\n",
      "iteration 6160: loss 0.615393\n",
      "iteration 6170: loss 0.615393\n",
      "iteration 6180: loss 0.615393\n",
      "iteration 6190: loss 0.615393\n",
      "iteration 6200: loss 0.615393\n",
      "iteration 6210: loss 0.615393\n",
      "iteration 6220: loss 0.615393\n",
      "iteration 6230: loss 0.615393\n",
      "iteration 6240: loss 0.615393\n",
      "iteration 6250: loss 0.615393\n",
      "iteration 6260: loss 0.615393\n",
      "iteration 6270: loss 0.615393\n",
      "iteration 6280: loss 0.615393\n",
      "iteration 6290: loss 0.615393\n",
      "iteration 6300: loss 0.615393\n",
      "iteration 6310: loss 0.615393\n",
      "iteration 6320: loss 0.615393\n",
      "iteration 6330: loss 0.615393\n",
      "iteration 6340: loss 0.615393\n",
      "iteration 6350: loss 0.615393\n",
      "iteration 6360: loss 0.615393\n",
      "iteration 6370: loss 0.615393\n",
      "iteration 6380: loss 0.615393\n",
      "iteration 6390: loss 0.615393\n",
      "iteration 6400: loss 0.615393\n",
      "iteration 6410: loss 0.615393\n",
      "iteration 6420: loss 0.615393\n",
      "iteration 6430: loss 0.615393\n",
      "iteration 6440: loss 0.615393\n",
      "iteration 6450: loss 0.615393\n",
      "iteration 6460: loss 0.615393\n",
      "iteration 6470: loss 0.615393\n",
      "iteration 6480: loss 0.615393\n",
      "iteration 6490: loss 0.615393\n",
      "iteration 6500: loss 0.615393\n",
      "iteration 6510: loss 0.615393\n",
      "iteration 6520: loss 0.615393\n",
      "iteration 6530: loss 0.615393\n",
      "iteration 6540: loss 0.615393\n",
      "iteration 6550: loss 0.615393\n",
      "iteration 6560: loss 0.615393\n",
      "iteration 6570: loss 0.615393\n",
      "iteration 6580: loss 0.615393\n",
      "iteration 6590: loss 0.615393\n",
      "iteration 6600: loss 0.615393\n",
      "iteration 6610: loss 0.615393\n",
      "iteration 6620: loss 0.615393\n",
      "iteration 6630: loss 0.615393\n",
      "iteration 6640: loss 0.615393\n",
      "iteration 6650: loss 0.615393\n",
      "iteration 6660: loss 0.615393\n",
      "iteration 6670: loss 0.615393\n",
      "iteration 6680: loss 0.615393\n",
      "iteration 6690: loss 0.615393\n",
      "iteration 6700: loss 0.615393\n",
      "iteration 6710: loss 0.615393\n",
      "iteration 6720: loss 0.615393\n",
      "iteration 6730: loss 0.615393\n",
      "iteration 6740: loss 0.615393\n",
      "iteration 6750: loss 0.615393\n",
      "iteration 6760: loss 0.615393\n",
      "iteration 6770: loss 0.615393\n",
      "iteration 6780: loss 0.615393\n",
      "iteration 6790: loss 0.615393\n",
      "iteration 6800: loss 0.615393\n",
      "iteration 6810: loss 0.615393\n",
      "iteration 6820: loss 0.615393\n",
      "iteration 6830: loss 0.615393\n",
      "iteration 6840: loss 0.615393\n",
      "iteration 6850: loss 0.615393\n",
      "iteration 6860: loss 0.615393\n",
      "iteration 6870: loss 0.615393\n",
      "iteration 6880: loss 0.615393\n",
      "iteration 6890: loss 0.615393\n",
      "iteration 6900: loss 0.615393\n",
      "iteration 6910: loss 0.615393\n",
      "iteration 6920: loss 0.615393\n",
      "iteration 6930: loss 0.615393\n",
      "iteration 6940: loss 0.615393\n",
      "iteration 6950: loss 0.615393\n",
      "iteration 6960: loss 0.615393\n",
      "iteration 6970: loss 0.615393\n",
      "iteration 6980: loss 0.615393\n",
      "iteration 6990: loss 0.615393\n",
      "iteration 7000: loss 0.615393\n",
      "iteration 7010: loss 0.615393\n",
      "iteration 7020: loss 0.615393\n",
      "iteration 7030: loss 0.615393\n",
      "iteration 7040: loss 0.615393\n",
      "iteration 7050: loss 0.615393\n",
      "iteration 7060: loss 0.615393\n",
      "iteration 7070: loss 0.615393\n",
      "iteration 7080: loss 0.615393\n",
      "iteration 7090: loss 0.615393\n",
      "iteration 7100: loss 0.615393\n",
      "iteration 7110: loss 0.615393\n",
      "iteration 7120: loss 0.615393\n",
      "iteration 7130: loss 0.615393\n",
      "iteration 7140: loss 0.615393\n",
      "iteration 7150: loss 0.615393\n",
      "iteration 7160: loss 0.615393\n",
      "iteration 7170: loss 0.615393\n",
      "iteration 7180: loss 0.615393\n",
      "iteration 7190: loss 0.615393\n",
      "iteration 7200: loss 0.615393\n",
      "iteration 7210: loss 0.615393\n",
      "iteration 7220: loss 0.615393\n",
      "iteration 7230: loss 0.615393\n",
      "iteration 7240: loss 0.615393\n",
      "iteration 7250: loss 0.615393\n",
      "iteration 7260: loss 0.615393\n",
      "iteration 7270: loss 0.615393\n",
      "iteration 7280: loss 0.615393\n",
      "iteration 7290: loss 0.615393\n",
      "iteration 7300: loss 0.615393\n",
      "iteration 7310: loss 0.615393\n",
      "iteration 7320: loss 0.615393\n",
      "iteration 7330: loss 0.615393\n",
      "iteration 7340: loss 0.615393\n",
      "iteration 7350: loss 0.615393\n",
      "iteration 7360: loss 0.615393\n",
      "iteration 7370: loss 0.615393\n",
      "iteration 7380: loss 0.615393\n",
      "iteration 7390: loss 0.615393\n",
      "iteration 7400: loss 0.615393\n",
      "iteration 7410: loss 0.615393\n",
      "iteration 7420: loss 0.615393\n",
      "iteration 7430: loss 0.615393\n",
      "iteration 7440: loss 0.615393\n",
      "iteration 7450: loss 0.615393\n",
      "iteration 7460: loss 0.615393\n",
      "iteration 7470: loss 0.615393\n",
      "iteration 7480: loss 0.615393\n",
      "iteration 7490: loss 0.615393\n",
      "iteration 7500: loss 0.615393\n",
      "iteration 7510: loss 0.615393\n",
      "iteration 7520: loss 0.615393\n",
      "iteration 7530: loss 0.615393\n",
      "iteration 7540: loss 0.615393\n",
      "iteration 7550: loss 0.615393\n",
      "iteration 7560: loss 0.615393\n",
      "iteration 7570: loss 0.615393\n",
      "iteration 7580: loss 0.615393\n",
      "iteration 7590: loss 0.615393\n",
      "iteration 7600: loss 0.615393\n",
      "iteration 7610: loss 0.615393\n",
      "iteration 7620: loss 0.615393\n",
      "iteration 7630: loss 0.615393\n",
      "iteration 7640: loss 0.615393\n",
      "iteration 7650: loss 0.615393\n",
      "iteration 7660: loss 0.615393\n",
      "iteration 7670: loss 0.615393\n",
      "iteration 7680: loss 0.615393\n",
      "iteration 7690: loss 0.615393\n",
      "iteration 7700: loss 0.615393\n",
      "iteration 7710: loss 0.615393\n",
      "iteration 7720: loss 0.615393\n",
      "iteration 7730: loss 0.615393\n",
      "iteration 7740: loss 0.615393\n",
      "iteration 7750: loss 0.615393\n",
      "iteration 7760: loss 0.615393\n",
      "iteration 7770: loss 0.615393\n",
      "iteration 7780: loss 0.615393\n",
      "iteration 7790: loss 0.615393\n",
      "iteration 7800: loss 0.615393\n",
      "iteration 7810: loss 0.615393\n",
      "iteration 7820: loss 0.615393\n",
      "iteration 7830: loss 0.615393\n",
      "iteration 7840: loss 0.615393\n",
      "iteration 7850: loss 0.615393\n",
      "iteration 7860: loss 0.615393\n",
      "iteration 7870: loss 0.615393\n",
      "iteration 7880: loss 0.615393\n",
      "iteration 7890: loss 0.615393\n",
      "iteration 7900: loss 0.615393\n",
      "iteration 7910: loss 0.615393\n",
      "iteration 7920: loss 0.615393\n",
      "iteration 7930: loss 0.615393\n",
      "iteration 7940: loss 0.615393\n",
      "iteration 7950: loss 0.615393\n",
      "iteration 7960: loss 0.615393\n",
      "iteration 7970: loss 0.615393\n",
      "iteration 7980: loss 0.615393\n",
      "iteration 7990: loss 0.615393\n",
      "iteration 8000: loss 0.615393\n",
      "iteration 8010: loss 0.615393\n",
      "iteration 8020: loss 0.615393\n",
      "iteration 8030: loss 0.615393\n",
      "iteration 8040: loss 0.615393\n",
      "iteration 8050: loss 0.615393\n",
      "iteration 8060: loss 0.615393\n",
      "iteration 8070: loss 0.615393\n",
      "iteration 8080: loss 0.615393\n",
      "iteration 8090: loss 0.615393\n",
      "iteration 8100: loss 0.615393\n",
      "iteration 8110: loss 0.615393\n",
      "iteration 8120: loss 0.615393\n",
      "iteration 8130: loss 0.615393\n",
      "iteration 8140: loss 0.615393\n",
      "iteration 8150: loss 0.615393\n",
      "iteration 8160: loss 0.615393\n",
      "iteration 8170: loss 0.615393\n",
      "iteration 8180: loss 0.615393\n",
      "iteration 8190: loss 0.615393\n",
      "iteration 8200: loss 0.615393\n",
      "iteration 8210: loss 0.615393\n",
      "iteration 8220: loss 0.615393\n",
      "iteration 8230: loss 0.615393\n",
      "iteration 8240: loss 0.615393\n",
      "iteration 8250: loss 0.615393\n",
      "iteration 8260: loss 0.615393\n",
      "iteration 8270: loss 0.615393\n",
      "iteration 8280: loss 0.615393\n",
      "iteration 8290: loss 0.615393\n",
      "iteration 8300: loss 0.615393\n",
      "iteration 8310: loss 0.615393\n",
      "iteration 8320: loss 0.615393\n",
      "iteration 8330: loss 0.615393\n",
      "iteration 8340: loss 0.615393\n",
      "iteration 8350: loss 0.615393\n",
      "iteration 8360: loss 0.615393\n",
      "iteration 8370: loss 0.615393\n",
      "iteration 8380: loss 0.615393\n",
      "iteration 8390: loss 0.615393\n",
      "iteration 8400: loss 0.615393\n",
      "iteration 8410: loss 0.615393\n",
      "iteration 8420: loss 0.615393\n",
      "iteration 8430: loss 0.615393\n",
      "iteration 8440: loss 0.615393\n",
      "iteration 8450: loss 0.615393\n",
      "iteration 8460: loss 0.615393\n",
      "iteration 8470: loss 0.615393\n",
      "iteration 8480: loss 0.615393\n",
      "iteration 8490: loss 0.615393\n",
      "iteration 8500: loss 0.615393\n",
      "iteration 8510: loss 0.615393\n",
      "iteration 8520: loss 0.615393\n",
      "iteration 8530: loss 0.615393\n",
      "iteration 8540: loss 0.615393\n",
      "iteration 8550: loss 0.615393\n",
      "iteration 8560: loss 0.615393\n",
      "iteration 8570: loss 0.615393\n",
      "iteration 8580: loss 0.615393\n",
      "iteration 8590: loss 0.615393\n",
      "iteration 8600: loss 0.615393\n",
      "iteration 8610: loss 0.615393\n",
      "iteration 8620: loss 0.615393\n",
      "iteration 8630: loss 0.615393\n",
      "iteration 8640: loss 0.615393\n",
      "iteration 8650: loss 0.615393\n",
      "iteration 8660: loss 0.615393\n",
      "iteration 8670: loss 0.615393\n",
      "iteration 8680: loss 0.615393\n",
      "iteration 8690: loss 0.615393\n",
      "iteration 8700: loss 0.615393\n",
      "iteration 8710: loss 0.615393\n",
      "iteration 8720: loss 0.615393\n",
      "iteration 8730: loss 0.615393\n",
      "iteration 8740: loss 0.615393\n",
      "iteration 8750: loss 0.615393\n",
      "iteration 8760: loss 0.615393\n",
      "iteration 8770: loss 0.615393\n",
      "iteration 8780: loss 0.615393\n",
      "iteration 8790: loss 0.615393\n",
      "iteration 8800: loss 0.615393\n",
      "iteration 8810: loss 0.615393\n",
      "iteration 8820: loss 0.615393\n",
      "iteration 8830: loss 0.615393\n",
      "iteration 8840: loss 0.615393\n",
      "iteration 8850: loss 0.615393\n",
      "iteration 8860: loss 0.615393\n",
      "iteration 8870: loss 0.615393\n",
      "iteration 8880: loss 0.615393\n",
      "iteration 8890: loss 0.615393\n",
      "iteration 8900: loss 0.615393\n",
      "iteration 8910: loss 0.615393\n",
      "iteration 8920: loss 0.615393\n",
      "iteration 8930: loss 0.615393\n",
      "iteration 8940: loss 0.615393\n",
      "iteration 8950: loss 0.615393\n",
      "iteration 8960: loss 0.615393\n",
      "iteration 8970: loss 0.615393\n",
      "iteration 8980: loss 0.615393\n",
      "iteration 8990: loss 0.615393\n",
      "iteration 9000: loss 0.615393\n",
      "iteration 9010: loss 0.615393\n",
      "iteration 9020: loss 0.615393\n",
      "iteration 9030: loss 0.615393\n",
      "iteration 9040: loss 0.615393\n",
      "iteration 9050: loss 0.615393\n",
      "iteration 9060: loss 0.615393\n",
      "iteration 9070: loss 0.615393\n",
      "iteration 9080: loss 0.615393\n",
      "iteration 9090: loss 0.615393\n",
      "iteration 9100: loss 0.615393\n",
      "iteration 9110: loss 0.615393\n",
      "iteration 9120: loss 0.615393\n",
      "iteration 9130: loss 0.615393\n",
      "iteration 9140: loss 0.615393\n",
      "iteration 9150: loss 0.615393\n",
      "iteration 9160: loss 0.615393\n",
      "iteration 9170: loss 0.615393\n",
      "iteration 9180: loss 0.615393\n",
      "iteration 9190: loss 0.615393\n",
      "iteration 9200: loss 0.615393\n",
      "iteration 9210: loss 0.615393\n",
      "iteration 9220: loss 0.615393\n",
      "iteration 9230: loss 0.615393\n",
      "iteration 9240: loss 0.615393\n",
      "iteration 9250: loss 0.615393\n",
      "iteration 9260: loss 0.615393\n",
      "iteration 9270: loss 0.615393\n",
      "iteration 9280: loss 0.615393\n",
      "iteration 9290: loss 0.615393\n",
      "iteration 9300: loss 0.615393\n",
      "iteration 9310: loss 0.615393\n",
      "iteration 9320: loss 0.615393\n",
      "iteration 9330: loss 0.615393\n",
      "iteration 9340: loss 0.615393\n",
      "iteration 9350: loss 0.615393\n",
      "iteration 9360: loss 0.615393\n",
      "iteration 9370: loss 0.615393\n",
      "iteration 9380: loss 0.615393\n",
      "iteration 9390: loss 0.615393\n",
      "iteration 9400: loss 0.615393\n",
      "iteration 9410: loss 0.615393\n",
      "iteration 9420: loss 0.615393\n",
      "iteration 9430: loss 0.615393\n",
      "iteration 9440: loss 0.615393\n",
      "iteration 9450: loss 0.615393\n",
      "iteration 9460: loss 0.615393\n",
      "iteration 9470: loss 0.615393\n",
      "iteration 9480: loss 0.615393\n",
      "iteration 9490: loss 0.615393\n",
      "iteration 9500: loss 0.615393\n",
      "iteration 9510: loss 0.615393\n",
      "iteration 9520: loss 0.615393\n",
      "iteration 9530: loss 0.615393\n",
      "iteration 9540: loss 0.615393\n",
      "iteration 9550: loss 0.615393\n",
      "iteration 9560: loss 0.615393\n",
      "iteration 9570: loss 0.615393\n",
      "iteration 9580: loss 0.615393\n",
      "iteration 9590: loss 0.615393\n",
      "iteration 9600: loss 0.615393\n",
      "iteration 9610: loss 0.615393\n",
      "iteration 9620: loss 0.615393\n",
      "iteration 9630: loss 0.615393\n",
      "iteration 9640: loss 0.615393\n",
      "iteration 9650: loss 0.615393\n",
      "iteration 9660: loss 0.615393\n",
      "iteration 9670: loss 0.615393\n",
      "iteration 9680: loss 0.615393\n",
      "iteration 9690: loss 0.615393\n",
      "iteration 9700: loss 0.615393\n",
      "iteration 9710: loss 0.615393\n",
      "iteration 9720: loss 0.615393\n",
      "iteration 9730: loss 0.615393\n",
      "iteration 9740: loss 0.615393\n",
      "iteration 9750: loss 0.615393\n",
      "iteration 9760: loss 0.615393\n",
      "iteration 9770: loss 0.615393\n",
      "iteration 9780: loss 0.615393\n",
      "iteration 9790: loss 0.615393\n",
      "iteration 9800: loss 0.615393\n",
      "iteration 9810: loss 0.615393\n",
      "iteration 9820: loss 0.615393\n",
      "iteration 9830: loss 0.615393\n",
      "iteration 9840: loss 0.615393\n",
      "iteration 9850: loss 0.615393\n",
      "iteration 9860: loss 0.615393\n",
      "iteration 9870: loss 0.615393\n",
      "iteration 9880: loss 0.615393\n",
      "iteration 9890: loss 0.615393\n",
      "iteration 9900: loss 0.615393\n",
      "iteration 9910: loss 0.615393\n",
      "iteration 9920: loss 0.615393\n",
      "iteration 9930: loss 0.615393\n",
      "iteration 9940: loss 0.615393\n",
      "iteration 9950: loss 0.615393\n",
      "iteration 9960: loss 0.615393\n",
      "iteration 9970: loss 0.615393\n",
      "iteration 9980: loss 0.615393\n",
      "iteration 9990: loss 0.615393\n",
      "iteration 10000: loss 0.615393\n",
      "iteration 10010: loss 0.615393\n",
      "iteration 10020: loss 0.615393\n",
      "iteration 10030: loss 0.615393\n",
      "iteration 10040: loss 0.615393\n",
      "iteration 10050: loss 0.615393\n",
      "iteration 10060: loss 0.615393\n",
      "iteration 10070: loss 0.615393\n",
      "iteration 10080: loss 0.615393\n",
      "iteration 10090: loss 0.615393\n",
      "iteration 10100: loss 0.615393\n",
      "iteration 10110: loss 0.615393\n",
      "iteration 10120: loss 0.615393\n",
      "iteration 10130: loss 0.615393\n",
      "iteration 10140: loss 0.615393\n",
      "iteration 10150: loss 0.615393\n",
      "iteration 10160: loss 0.615393\n",
      "iteration 10170: loss 0.615393\n",
      "iteration 10180: loss 0.615393\n",
      "iteration 10190: loss 0.615393\n",
      "iteration 10200: loss 0.615393\n",
      "iteration 10210: loss 0.615393\n",
      "iteration 10220: loss 0.615393\n",
      "iteration 10230: loss 0.615393\n",
      "iteration 10240: loss 0.615393\n",
      "iteration 10250: loss 0.615393\n",
      "iteration 10260: loss 0.615393\n",
      "iteration 10270: loss 0.615393\n",
      "iteration 10280: loss 0.615393\n",
      "iteration 10290: loss 0.615393\n",
      "iteration 10300: loss 0.615393\n",
      "iteration 10310: loss 0.615393\n",
      "iteration 10320: loss 0.615393\n",
      "iteration 10330: loss 0.615393\n",
      "iteration 10340: loss 0.615393\n",
      "iteration 10350: loss 0.615393\n",
      "iteration 10360: loss 0.615393\n",
      "iteration 10370: loss 0.615393\n",
      "iteration 10380: loss 0.615393\n",
      "iteration 10390: loss 0.615393\n",
      "iteration 10400: loss 0.615393\n",
      "iteration 10410: loss 0.615393\n",
      "iteration 10420: loss 0.615393\n",
      "iteration 10430: loss 0.615393\n",
      "iteration 10440: loss 0.615393\n",
      "iteration 10450: loss 0.615393\n",
      "iteration 10460: loss 0.615393\n",
      "iteration 10470: loss 0.615393\n",
      "iteration 10480: loss 0.615393\n",
      "iteration 10490: loss 0.615393\n",
      "iteration 10500: loss 0.615393\n",
      "iteration 10510: loss 0.615393\n",
      "iteration 10520: loss 0.615393\n",
      "iteration 10530: loss 0.615393\n",
      "iteration 10540: loss 0.615393\n",
      "iteration 10550: loss 0.615393\n",
      "iteration 10560: loss 0.615393\n",
      "iteration 10570: loss 0.615393\n",
      "iteration 10580: loss 0.615393\n",
      "iteration 10590: loss 0.615393\n",
      "iteration 10600: loss 0.615393\n",
      "iteration 10610: loss 0.615393\n",
      "iteration 10620: loss 0.615393\n",
      "iteration 10630: loss 0.615393\n",
      "iteration 10640: loss 0.615393\n",
      "iteration 10650: loss 0.615393\n",
      "iteration 10660: loss 0.615393\n",
      "iteration 10670: loss 0.615393\n",
      "iteration 10680: loss 0.615393\n",
      "iteration 10690: loss 0.615393\n",
      "iteration 10700: loss 0.615393\n",
      "iteration 10710: loss 0.615393\n",
      "iteration 10720: loss 0.615393\n",
      "iteration 10730: loss 0.615393\n",
      "iteration 10740: loss 0.615393\n",
      "iteration 10750: loss 0.615393\n",
      "iteration 10760: loss 0.615393\n",
      "iteration 10770: loss 0.615393\n",
      "iteration 10780: loss 0.615393\n",
      "iteration 10790: loss 0.615393\n",
      "iteration 10800: loss 0.615393\n",
      "iteration 10810: loss 0.615393\n",
      "iteration 10820: loss 0.615393\n",
      "iteration 10830: loss 0.615393\n",
      "iteration 10840: loss 0.615393\n",
      "iteration 10850: loss 0.615393\n",
      "iteration 10860: loss 0.615393\n",
      "iteration 10870: loss 0.615393\n",
      "iteration 10880: loss 0.615393\n",
      "iteration 10890: loss 0.615393\n",
      "iteration 10900: loss 0.615393\n",
      "iteration 10910: loss 0.615393\n",
      "iteration 10920: loss 0.615393\n",
      "iteration 10930: loss 0.615393\n",
      "iteration 10940: loss 0.615393\n",
      "iteration 10950: loss 0.615393\n",
      "iteration 10960: loss 0.615393\n",
      "iteration 10970: loss 0.615393\n",
      "iteration 10980: loss 0.615393\n",
      "iteration 10990: loss 0.615393\n",
      "iteration 11000: loss 0.615393\n",
      "iteration 11010: loss 0.615393\n",
      "iteration 11020: loss 0.615393\n",
      "iteration 11030: loss 0.615393\n",
      "iteration 11040: loss 0.615393\n",
      "iteration 11050: loss 0.615393\n",
      "iteration 11060: loss 0.615393\n",
      "iteration 11070: loss 0.615393\n",
      "iteration 11080: loss 0.615393\n",
      "iteration 11090: loss 0.615393\n",
      "iteration 11100: loss 0.615393\n",
      "iteration 11110: loss 0.615393\n",
      "iteration 11120: loss 0.615393\n",
      "iteration 11130: loss 0.615393\n",
      "iteration 11140: loss 0.615393\n",
      "iteration 11150: loss 0.615393\n",
      "iteration 11160: loss 0.615393\n",
      "iteration 11170: loss 0.615393\n",
      "iteration 11180: loss 0.615393\n",
      "iteration 11190: loss 0.615393\n",
      "iteration 11200: loss 0.615393\n",
      "iteration 11210: loss 0.615393\n",
      "iteration 11220: loss 0.615393\n",
      "iteration 11230: loss 0.615393\n",
      "iteration 11240: loss 0.615393\n",
      "iteration 11250: loss 0.615393\n",
      "iteration 11260: loss 0.615393\n",
      "iteration 11270: loss 0.615393\n",
      "iteration 11280: loss 0.615393\n",
      "iteration 11290: loss 0.615393\n",
      "iteration 11300: loss 0.615393\n",
      "iteration 11310: loss 0.615393\n",
      "iteration 11320: loss 0.615393\n",
      "iteration 11330: loss 0.615393\n",
      "iteration 11340: loss 0.615393\n",
      "iteration 11350: loss 0.615393\n",
      "iteration 11360: loss 0.615393\n",
      "iteration 11370: loss 0.615393\n",
      "iteration 11380: loss 0.615393\n",
      "iteration 11390: loss 0.615393\n",
      "iteration 11400: loss 0.615393\n",
      "iteration 11410: loss 0.615393\n",
      "iteration 11420: loss 0.615393\n",
      "iteration 11430: loss 0.615393\n",
      "iteration 11440: loss 0.615393\n",
      "iteration 11450: loss 0.615393\n",
      "iteration 11460: loss 0.615393\n",
      "iteration 11470: loss 0.615393\n",
      "iteration 11480: loss 0.615393\n",
      "iteration 11490: loss 0.615393\n",
      "iteration 11500: loss 0.615393\n",
      "iteration 11510: loss 0.615393\n",
      "iteration 11520: loss 0.615393\n",
      "iteration 11530: loss 0.615393\n",
      "iteration 11540: loss 0.615393\n",
      "iteration 11550: loss 0.615393\n",
      "iteration 11560: loss 0.615393\n",
      "iteration 11570: loss 0.615393\n",
      "iteration 11580: loss 0.615393\n",
      "iteration 11590: loss 0.615393\n",
      "iteration 11600: loss 0.615393\n",
      "iteration 11610: loss 0.615393\n",
      "iteration 11620: loss 0.615393\n",
      "iteration 11630: loss 0.615393\n",
      "iteration 11640: loss 0.615393\n",
      "iteration 11650: loss 0.615393\n",
      "iteration 11660: loss 0.615393\n",
      "iteration 11670: loss 0.615393\n",
      "iteration 11680: loss 0.615393\n",
      "iteration 11690: loss 0.615393\n",
      "iteration 11700: loss 0.615393\n",
      "iteration 11710: loss 0.615393\n",
      "iteration 11720: loss 0.615393\n",
      "iteration 11730: loss 0.615393\n",
      "iteration 11740: loss 0.615393\n",
      "iteration 11750: loss 0.615393\n",
      "iteration 11760: loss 0.615393\n",
      "iteration 11770: loss 0.615393\n",
      "iteration 11780: loss 0.615393\n",
      "iteration 11790: loss 0.615393\n",
      "iteration 11800: loss 0.615393\n",
      "iteration 11810: loss 0.615393\n",
      "iteration 11820: loss 0.615393\n",
      "iteration 11830: loss 0.615393\n",
      "iteration 11840: loss 0.615393\n",
      "iteration 11850: loss 0.615393\n",
      "iteration 11860: loss 0.615393\n",
      "iteration 11870: loss 0.615393\n",
      "iteration 11880: loss 0.615393\n",
      "iteration 11890: loss 0.615393\n",
      "iteration 11900: loss 0.615393\n",
      "iteration 11910: loss 0.615393\n",
      "iteration 11920: loss 0.615393\n",
      "iteration 11930: loss 0.615393\n",
      "iteration 11940: loss 0.615393\n",
      "iteration 11950: loss 0.615393\n",
      "iteration 11960: loss 0.615393\n",
      "iteration 11970: loss 0.615393\n",
      "iteration 11980: loss 0.615393\n",
      "iteration 11990: loss 0.615393\n",
      "iteration 12000: loss 0.615393\n",
      "iteration 12010: loss 0.615393\n",
      "iteration 12020: loss 0.615393\n",
      "iteration 12030: loss 0.615393\n",
      "iteration 12040: loss 0.615393\n",
      "iteration 12050: loss 0.615393\n",
      "iteration 12060: loss 0.615393\n",
      "iteration 12070: loss 0.615393\n",
      "iteration 12080: loss 0.615393\n",
      "iteration 12090: loss 0.615393\n",
      "iteration 12100: loss 0.615393\n",
      "iteration 12110: loss 0.615393\n",
      "iteration 12120: loss 0.615393\n",
      "iteration 12130: loss 0.615393\n",
      "iteration 12140: loss 0.615393\n",
      "iteration 12150: loss 0.615393\n",
      "iteration 12160: loss 0.615393\n",
      "iteration 12170: loss 0.615393\n",
      "iteration 12180: loss 0.615393\n",
      "iteration 12190: loss 0.615393\n",
      "iteration 12200: loss 0.615393\n",
      "iteration 12210: loss 0.615393\n",
      "iteration 12220: loss 0.615393\n",
      "iteration 12230: loss 0.615393\n",
      "iteration 12240: loss 0.615393\n",
      "iteration 12250: loss 0.615393\n",
      "iteration 12260: loss 0.615393\n",
      "iteration 12270: loss 0.615393\n",
      "iteration 12280: loss 0.615393\n",
      "iteration 12290: loss 0.615393\n",
      "iteration 12300: loss 0.615393\n",
      "iteration 12310: loss 0.615393\n",
      "iteration 12320: loss 0.615393\n",
      "iteration 12330: loss 0.615393\n",
      "iteration 12340: loss 0.615393\n",
      "iteration 12350: loss 0.615393\n",
      "iteration 12360: loss 0.615393\n",
      "iteration 12370: loss 0.615393\n",
      "iteration 12380: loss 0.615393\n",
      "iteration 12390: loss 0.615393\n",
      "iteration 12400: loss 0.615393\n",
      "iteration 12410: loss 0.615393\n",
      "iteration 12420: loss 0.615393\n",
      "iteration 12430: loss 0.615393\n",
      "iteration 12440: loss 0.615393\n",
      "iteration 12450: loss 0.615393\n",
      "iteration 12460: loss 0.615393\n",
      "iteration 12470: loss 0.615393\n",
      "iteration 12480: loss 0.615393\n",
      "iteration 12490: loss 0.615393\n",
      "iteration 12500: loss 0.615393\n",
      "iteration 12510: loss 0.615393\n",
      "iteration 12520: loss 0.615393\n",
      "iteration 12530: loss 0.615393\n",
      "iteration 12540: loss 0.615393\n",
      "iteration 12550: loss 0.615393\n",
      "iteration 12560: loss 0.615393\n",
      "iteration 12570: loss 0.615393\n",
      "iteration 12580: loss 0.615393\n",
      "iteration 12590: loss 0.615393\n",
      "iteration 12600: loss 0.615393\n",
      "iteration 12610: loss 0.615393\n",
      "iteration 12620: loss 0.615393\n",
      "iteration 12630: loss 0.615393\n",
      "iteration 12640: loss 0.615393\n",
      "iteration 12650: loss 0.615393\n",
      "iteration 12660: loss 0.615393\n",
      "iteration 12670: loss 0.615393\n",
      "iteration 12680: loss 0.615393\n",
      "iteration 12690: loss 0.615393\n",
      "iteration 12700: loss 0.615393\n",
      "iteration 12710: loss 0.615393\n",
      "iteration 12720: loss 0.615393\n",
      "iteration 12730: loss 0.615393\n",
      "iteration 12740: loss 0.615393\n",
      "iteration 12750: loss 0.615393\n",
      "iteration 12760: loss 0.615393\n",
      "iteration 12770: loss 0.615393\n",
      "iteration 12780: loss 0.615393\n",
      "iteration 12790: loss 0.615393\n",
      "iteration 12800: loss 0.615393\n",
      "iteration 12810: loss 0.615393\n",
      "iteration 12820: loss 0.615393\n",
      "iteration 12830: loss 0.615393\n",
      "iteration 12840: loss 0.615393\n",
      "iteration 12850: loss 0.615393\n",
      "iteration 12860: loss 0.615393\n",
      "iteration 12870: loss 0.615393\n",
      "iteration 12880: loss 0.615393\n",
      "iteration 12890: loss 0.615393\n",
      "iteration 12900: loss 0.615393\n",
      "iteration 12910: loss 0.615393\n",
      "iteration 12920: loss 0.615393\n",
      "iteration 12930: loss 0.615393\n",
      "iteration 12940: loss 0.615393\n",
      "iteration 12950: loss 0.615393\n",
      "iteration 12960: loss 0.615393\n",
      "iteration 12970: loss 0.615393\n",
      "iteration 12980: loss 0.615393\n",
      "iteration 12990: loss 0.615393\n",
      "iteration 13000: loss 0.615393\n",
      "iteration 13010: loss 0.615393\n",
      "iteration 13020: loss 0.615393\n",
      "iteration 13030: loss 0.615393\n",
      "iteration 13040: loss 0.615393\n",
      "iteration 13050: loss 0.615393\n",
      "iteration 13060: loss 0.615393\n",
      "iteration 13070: loss 0.615393\n",
      "iteration 13080: loss 0.615393\n",
      "iteration 13090: loss 0.615393\n",
      "iteration 13100: loss 0.615393\n",
      "iteration 13110: loss 0.615393\n",
      "iteration 13120: loss 0.615393\n",
      "iteration 13130: loss 0.615393\n",
      "iteration 13140: loss 0.615393\n",
      "iteration 13150: loss 0.615393\n",
      "iteration 13160: loss 0.615393\n",
      "iteration 13170: loss 0.615393\n",
      "iteration 13180: loss 0.615393\n",
      "iteration 13190: loss 0.615393\n",
      "iteration 13200: loss 0.615393\n",
      "iteration 13210: loss 0.615393\n",
      "iteration 13220: loss 0.615393\n",
      "iteration 13230: loss 0.615393\n",
      "iteration 13240: loss 0.615393\n",
      "iteration 13250: loss 0.615393\n",
      "iteration 13260: loss 0.615393\n",
      "iteration 13270: loss 0.615393\n",
      "iteration 13280: loss 0.615393\n",
      "iteration 13290: loss 0.615393\n",
      "iteration 13300: loss 0.615393\n",
      "iteration 13310: loss 0.615393\n",
      "iteration 13320: loss 0.615393\n",
      "iteration 13330: loss 0.615393\n",
      "iteration 13340: loss 0.615393\n",
      "iteration 13350: loss 0.615393\n",
      "iteration 13360: loss 0.615393\n",
      "iteration 13370: loss 0.615393\n",
      "iteration 13380: loss 0.615393\n",
      "iteration 13390: loss 0.615393\n",
      "iteration 13400: loss 0.615393\n",
      "iteration 13410: loss 0.615393\n",
      "iteration 13420: loss 0.615393\n",
      "iteration 13430: loss 0.615393\n",
      "iteration 13440: loss 0.615393\n",
      "iteration 13450: loss 0.615393\n",
      "iteration 13460: loss 0.615393\n",
      "iteration 13470: loss 0.615393\n",
      "iteration 13480: loss 0.615393\n",
      "iteration 13490: loss 0.615393\n",
      "iteration 13500: loss 0.615393\n",
      "iteration 13510: loss 0.615393\n",
      "iteration 13520: loss 0.615393\n",
      "iteration 13530: loss 0.615393\n",
      "iteration 13540: loss 0.615393\n",
      "iteration 13550: loss 0.615393\n",
      "iteration 13560: loss 0.615393\n",
      "iteration 13570: loss 0.615393\n",
      "iteration 13580: loss 0.615393\n",
      "iteration 13590: loss 0.615393\n",
      "iteration 13600: loss 0.615393\n",
      "iteration 13610: loss 0.615393\n",
      "iteration 13620: loss 0.615393\n",
      "iteration 13630: loss 0.615393\n",
      "iteration 13640: loss 0.615393\n",
      "iteration 13650: loss 0.615393\n",
      "iteration 13660: loss 0.615393\n",
      "iteration 13670: loss 0.615393\n",
      "iteration 13680: loss 0.615393\n",
      "iteration 13690: loss 0.615393\n",
      "iteration 13700: loss 0.615393\n",
      "iteration 13710: loss 0.615393\n",
      "iteration 13720: loss 0.615393\n",
      "iteration 13730: loss 0.615393\n",
      "iteration 13740: loss 0.615393\n",
      "iteration 13750: loss 0.615393\n",
      "iteration 13760: loss 0.615393\n",
      "iteration 13770: loss 0.615393\n",
      "iteration 13780: loss 0.615393\n",
      "iteration 13790: loss 0.615393\n",
      "iteration 13800: loss 0.615393\n",
      "iteration 13810: loss 0.615393\n",
      "iteration 13820: loss 0.615393\n",
      "iteration 13830: loss 0.615393\n",
      "iteration 13840: loss 0.615393\n",
      "iteration 13850: loss 0.615393\n",
      "iteration 13860: loss 0.615393\n",
      "iteration 13870: loss 0.615393\n",
      "iteration 13880: loss 0.615393\n",
      "iteration 13890: loss 0.615393\n",
      "iteration 13900: loss 0.615393\n",
      "iteration 13910: loss 0.615393\n",
      "iteration 13920: loss 0.615393\n",
      "iteration 13930: loss 0.615393\n",
      "iteration 13940: loss 0.615393\n",
      "iteration 13950: loss 0.615393\n",
      "iteration 13960: loss 0.615393\n",
      "iteration 13970: loss 0.615393\n",
      "iteration 13980: loss 0.615393\n",
      "iteration 13990: loss 0.615393\n",
      "iteration 14000: loss 0.615393\n",
      "iteration 14010: loss 0.615393\n",
      "iteration 14020: loss 0.615393\n",
      "iteration 14030: loss 0.615393\n",
      "iteration 14040: loss 0.615393\n",
      "iteration 14050: loss 0.615393\n",
      "iteration 14060: loss 0.615393\n",
      "iteration 14070: loss 0.615393\n",
      "iteration 14080: loss 0.615393\n",
      "iteration 14090: loss 0.615393\n",
      "iteration 14100: loss 0.615393\n",
      "iteration 14110: loss 0.615393\n",
      "iteration 14120: loss 0.615393\n",
      "iteration 14130: loss 0.615393\n",
      "iteration 14140: loss 0.615393\n",
      "iteration 14150: loss 0.615393\n",
      "iteration 14160: loss 0.615393\n",
      "iteration 14170: loss 0.615393\n",
      "iteration 14180: loss 0.615393\n",
      "iteration 14190: loss 0.615393\n",
      "iteration 14200: loss 0.615393\n",
      "iteration 14210: loss 0.615393\n",
      "iteration 14220: loss 0.615393\n",
      "iteration 14230: loss 0.615393\n",
      "iteration 14240: loss 0.615393\n",
      "iteration 14250: loss 0.615393\n",
      "iteration 14260: loss 0.615393\n",
      "iteration 14270: loss 0.615393\n",
      "iteration 14280: loss 0.615393\n",
      "iteration 14290: loss 0.615393\n",
      "iteration 14300: loss 0.615393\n",
      "iteration 14310: loss 0.615393\n",
      "iteration 14320: loss 0.615393\n",
      "iteration 14330: loss 0.615393\n",
      "iteration 14340: loss 0.615393\n",
      "iteration 14350: loss 0.615393\n",
      "iteration 14360: loss 0.615393\n",
      "iteration 14370: loss 0.615393\n",
      "iteration 14380: loss 0.615393\n",
      "iteration 14390: loss 0.615393\n",
      "iteration 14400: loss 0.615393\n",
      "iteration 14410: loss 0.615393\n",
      "iteration 14420: loss 0.615393\n",
      "iteration 14430: loss 0.615393\n",
      "iteration 14440: loss 0.615393\n",
      "iteration 14450: loss 0.615393\n",
      "iteration 14460: loss 0.615393\n",
      "iteration 14470: loss 0.615393\n",
      "iteration 14480: loss 0.615393\n",
      "iteration 14490: loss 0.615393\n",
      "iteration 14500: loss 0.615393\n",
      "iteration 14510: loss 0.615393\n",
      "iteration 14520: loss 0.615393\n",
      "iteration 14530: loss 0.615393\n",
      "iteration 14540: loss 0.615393\n",
      "iteration 14550: loss 0.615393\n",
      "iteration 14560: loss 0.615393\n",
      "iteration 14570: loss 0.615393\n",
      "iteration 14580: loss 0.615393\n",
      "iteration 14590: loss 0.615393\n",
      "iteration 14600: loss 0.615393\n",
      "iteration 14610: loss 0.615393\n",
      "iteration 14620: loss 0.615393\n",
      "iteration 14630: loss 0.615393\n",
      "iteration 14640: loss 0.615393\n",
      "iteration 14650: loss 0.615393\n",
      "iteration 14660: loss 0.615393\n",
      "iteration 14670: loss 0.615393\n",
      "iteration 14680: loss 0.615393\n",
      "iteration 14690: loss 0.615393\n",
      "iteration 14700: loss 0.615393\n",
      "iteration 14710: loss 0.615393\n",
      "iteration 14720: loss 0.615393\n",
      "iteration 14730: loss 0.615393\n",
      "iteration 14740: loss 0.615393\n",
      "iteration 14750: loss 0.615393\n",
      "iteration 14760: loss 0.615393\n",
      "iteration 14770: loss 0.615393\n",
      "iteration 14780: loss 0.615393\n",
      "iteration 14790: loss 0.615393\n",
      "iteration 14800: loss 0.615393\n",
      "iteration 14810: loss 0.615393\n",
      "iteration 14820: loss 0.615393\n",
      "iteration 14830: loss 0.615393\n",
      "iteration 14840: loss 0.615393\n",
      "iteration 14850: loss 0.615393\n",
      "iteration 14860: loss 0.615393\n",
      "iteration 14870: loss 0.615393\n",
      "iteration 14880: loss 0.615393\n",
      "iteration 14890: loss 0.615393\n",
      "iteration 14900: loss 0.615393\n",
      "iteration 14910: loss 0.615393\n",
      "iteration 14920: loss 0.615393\n",
      "iteration 14930: loss 0.615393\n",
      "iteration 14940: loss 0.615393\n",
      "iteration 14950: loss 0.615393\n",
      "iteration 14960: loss 0.615393\n",
      "iteration 14970: loss 0.615393\n",
      "iteration 14980: loss 0.615393\n",
      "iteration 14990: loss 0.615393\n",
      "iteration 15000: loss 0.615393\n",
      "iteration 15010: loss 0.615393\n",
      "iteration 15020: loss 0.615393\n",
      "iteration 15030: loss 0.615393\n",
      "iteration 15040: loss 0.615393\n",
      "iteration 15050: loss 0.615393\n",
      "iteration 15060: loss 0.615393\n",
      "iteration 15070: loss 0.615393\n",
      "iteration 15080: loss 0.615393\n",
      "iteration 15090: loss 0.615393\n",
      "iteration 15100: loss 0.615393\n",
      "iteration 15110: loss 0.615393\n",
      "iteration 15120: loss 0.615393\n",
      "iteration 15130: loss 0.615393\n",
      "iteration 15140: loss 0.615393\n",
      "iteration 15150: loss 0.615393\n",
      "iteration 15160: loss 0.615393\n",
      "iteration 15170: loss 0.615393\n",
      "iteration 15180: loss 0.615393\n",
      "iteration 15190: loss 0.615393\n",
      "iteration 15200: loss 0.615393\n",
      "iteration 15210: loss 0.615393\n",
      "iteration 15220: loss 0.615393\n",
      "iteration 15230: loss 0.615393\n",
      "iteration 15240: loss 0.615393\n",
      "iteration 15250: loss 0.615393\n",
      "iteration 15260: loss 0.615393\n",
      "iteration 15270: loss 0.615393\n",
      "iteration 15280: loss 0.615393\n",
      "iteration 15290: loss 0.615393\n",
      "iteration 15300: loss 0.615393\n",
      "iteration 15310: loss 0.615393\n",
      "iteration 15320: loss 0.615393\n",
      "iteration 15330: loss 0.615393\n",
      "iteration 15340: loss 0.615393\n",
      "iteration 15350: loss 0.615393\n",
      "iteration 15360: loss 0.615393\n",
      "iteration 15370: loss 0.615393\n",
      "iteration 15380: loss 0.615393\n",
      "iteration 15390: loss 0.615393\n",
      "iteration 15400: loss 0.615393\n",
      "iteration 15410: loss 0.615393\n",
      "iteration 15420: loss 0.615393\n",
      "iteration 15430: loss 0.615393\n",
      "iteration 15440: loss 0.615393\n",
      "iteration 15450: loss 0.615393\n",
      "iteration 15460: loss 0.615393\n",
      "iteration 15470: loss 0.615393\n",
      "iteration 15480: loss 0.615393\n",
      "iteration 15490: loss 0.615393\n",
      "iteration 15500: loss 0.615393\n",
      "iteration 15510: loss 0.615393\n",
      "iteration 15520: loss 0.615393\n",
      "iteration 15530: loss 0.615393\n",
      "iteration 15540: loss 0.615393\n",
      "iteration 15550: loss 0.615393\n",
      "iteration 15560: loss 0.615393\n",
      "iteration 15570: loss 0.615393\n",
      "iteration 15580: loss 0.615393\n",
      "iteration 15590: loss 0.615393\n",
      "iteration 15600: loss 0.615393\n",
      "iteration 15610: loss 0.615393\n",
      "iteration 15620: loss 0.615393\n",
      "iteration 15630: loss 0.615393\n",
      "iteration 15640: loss 0.615393\n",
      "iteration 15650: loss 0.615393\n",
      "iteration 15660: loss 0.615393\n",
      "iteration 15670: loss 0.615393\n",
      "iteration 15680: loss 0.615393\n",
      "iteration 15690: loss 0.615393\n",
      "iteration 15700: loss 0.615393\n",
      "iteration 15710: loss 0.615393\n",
      "iteration 15720: loss 0.615393\n",
      "iteration 15730: loss 0.615393\n",
      "iteration 15740: loss 0.615393\n",
      "iteration 15750: loss 0.615393\n",
      "iteration 15760: loss 0.615393\n",
      "iteration 15770: loss 0.615393\n",
      "iteration 15780: loss 0.615393\n",
      "iteration 15790: loss 0.615393\n",
      "iteration 15800: loss 0.615393\n",
      "iteration 15810: loss 0.615393\n",
      "iteration 15820: loss 0.615393\n",
      "iteration 15830: loss 0.615393\n",
      "iteration 15840: loss 0.615393\n",
      "iteration 15850: loss 0.615393\n",
      "iteration 15860: loss 0.615393\n",
      "iteration 15870: loss 0.615393\n",
      "iteration 15880: loss 0.615393\n",
      "iteration 15890: loss 0.615393\n",
      "iteration 15900: loss 0.615393\n",
      "iteration 15910: loss 0.615393\n",
      "iteration 15920: loss 0.615393\n",
      "iteration 15930: loss 0.615393\n",
      "iteration 15940: loss 0.615393\n",
      "iteration 15950: loss 0.615393\n",
      "iteration 15960: loss 0.615393\n",
      "iteration 15970: loss 0.615393\n",
      "iteration 15980: loss 0.615393\n",
      "iteration 15990: loss 0.615393\n",
      "iteration 16000: loss 0.615393\n",
      "iteration 16010: loss 0.615393\n",
      "iteration 16020: loss 0.615393\n",
      "iteration 16030: loss 0.615393\n",
      "iteration 16040: loss 0.615393\n",
      "iteration 16050: loss 0.615393\n",
      "iteration 16060: loss 0.615393\n",
      "iteration 16070: loss 0.615393\n",
      "iteration 16080: loss 0.615393\n",
      "iteration 16090: loss 0.615393\n",
      "iteration 16100: loss 0.615393\n",
      "iteration 16110: loss 0.615393\n",
      "iteration 16120: loss 0.615393\n",
      "iteration 16130: loss 0.615393\n",
      "iteration 16140: loss 0.615393\n",
      "iteration 16150: loss 0.615393\n",
      "iteration 16160: loss 0.615393\n",
      "iteration 16170: loss 0.615393\n",
      "iteration 16180: loss 0.615393\n",
      "iteration 16190: loss 0.615393\n",
      "iteration 16200: loss 0.615393\n",
      "iteration 16210: loss 0.615393\n",
      "iteration 16220: loss 0.615393\n",
      "iteration 16230: loss 0.615393\n",
      "iteration 16240: loss 0.615393\n",
      "iteration 16250: loss 0.615393\n",
      "iteration 16260: loss 0.615393\n",
      "iteration 16270: loss 0.615393\n",
      "iteration 16280: loss 0.615393\n",
      "iteration 16290: loss 0.615393\n",
      "iteration 16300: loss 0.615393\n",
      "iteration 16310: loss 0.615393\n",
      "iteration 16320: loss 0.615393\n",
      "iteration 16330: loss 0.615393\n",
      "iteration 16340: loss 0.615393\n",
      "iteration 16350: loss 0.615393\n",
      "iteration 16360: loss 0.615393\n",
      "iteration 16370: loss 0.615393\n",
      "iteration 16380: loss 0.615393\n",
      "iteration 16390: loss 0.615393\n",
      "iteration 16400: loss 0.615393\n",
      "iteration 16410: loss 0.615393\n",
      "iteration 16420: loss 0.615393\n",
      "iteration 16430: loss 0.615393\n",
      "iteration 16440: loss 0.615393\n",
      "iteration 16450: loss 0.615393\n",
      "iteration 16460: loss 0.615393\n",
      "iteration 16470: loss 0.615393\n",
      "iteration 16480: loss 0.615393\n",
      "iteration 16490: loss 0.615393\n",
      "iteration 16500: loss 0.615393\n",
      "iteration 16510: loss 0.615393\n",
      "iteration 16520: loss 0.615393\n",
      "iteration 16530: loss 0.615393\n",
      "iteration 16540: loss 0.615393\n",
      "iteration 16550: loss 0.615393\n",
      "iteration 16560: loss 0.615393\n",
      "iteration 16570: loss 0.615393\n",
      "iteration 16580: loss 0.615393\n",
      "iteration 16590: loss 0.615393\n",
      "iteration 16600: loss 0.615393\n",
      "iteration 16610: loss 0.615393\n",
      "iteration 16620: loss 0.615393\n",
      "iteration 16630: loss 0.615393\n",
      "iteration 16640: loss 0.615393\n",
      "iteration 16650: loss 0.615393\n",
      "iteration 16660: loss 0.615393\n",
      "iteration 16670: loss 0.615393\n",
      "iteration 16680: loss 0.615393\n",
      "iteration 16690: loss 0.615393\n",
      "iteration 16700: loss 0.615393\n",
      "iteration 16710: loss 0.615393\n",
      "iteration 16720: loss 0.615393\n",
      "iteration 16730: loss 0.615393\n",
      "iteration 16740: loss 0.615393\n",
      "iteration 16750: loss 0.615393\n",
      "iteration 16760: loss 0.615393\n",
      "iteration 16770: loss 0.615393\n",
      "iteration 16780: loss 0.615393\n",
      "iteration 16790: loss 0.615393\n",
      "iteration 16800: loss 0.615393\n",
      "iteration 16810: loss 0.615393\n",
      "iteration 16820: loss 0.615393\n",
      "iteration 16830: loss 0.615393\n",
      "iteration 16840: loss 0.615393\n",
      "iteration 16850: loss 0.615393\n",
      "iteration 16860: loss 0.615393\n",
      "iteration 16870: loss 0.615393\n",
      "iteration 16880: loss 0.615393\n",
      "iteration 16890: loss 0.615393\n",
      "iteration 16900: loss 0.615393\n",
      "iteration 16910: loss 0.615393\n",
      "iteration 16920: loss 0.615393\n",
      "iteration 16930: loss 0.615393\n",
      "iteration 16940: loss 0.615393\n",
      "iteration 16950: loss 0.615393\n",
      "iteration 16960: loss 0.615393\n",
      "iteration 16970: loss 0.615393\n",
      "iteration 16980: loss 0.615393\n",
      "iteration 16990: loss 0.615393\n",
      "iteration 17000: loss 0.615393\n",
      "iteration 17010: loss 0.615393\n",
      "iteration 17020: loss 0.615393\n",
      "iteration 17030: loss 0.615393\n",
      "iteration 17040: loss 0.615393\n",
      "iteration 17050: loss 0.615393\n",
      "iteration 17060: loss 0.615393\n",
      "iteration 17070: loss 0.615393\n",
      "iteration 17080: loss 0.615393\n",
      "iteration 17090: loss 0.615393\n",
      "iteration 17100: loss 0.615393\n",
      "iteration 17110: loss 0.615393\n",
      "iteration 17120: loss 0.615393\n",
      "iteration 17130: loss 0.615393\n",
      "iteration 17140: loss 0.615393\n",
      "iteration 17150: loss 0.615393\n",
      "iteration 17160: loss 0.615393\n",
      "iteration 17170: loss 0.615393\n",
      "iteration 17180: loss 0.615393\n",
      "iteration 17190: loss 0.615393\n",
      "iteration 17200: loss 0.615393\n",
      "iteration 17210: loss 0.615393\n",
      "iteration 17220: loss 0.615393\n",
      "iteration 17230: loss 0.615393\n",
      "iteration 17240: loss 0.615393\n",
      "iteration 17250: loss 0.615393\n",
      "iteration 17260: loss 0.615393\n",
      "iteration 17270: loss 0.615393\n",
      "iteration 17280: loss 0.615393\n",
      "iteration 17290: loss 0.615393\n",
      "iteration 17300: loss 0.615393\n",
      "iteration 17310: loss 0.615393\n",
      "iteration 17320: loss 0.615393\n",
      "iteration 17330: loss 0.615393\n",
      "iteration 17340: loss 0.615393\n",
      "iteration 17350: loss 0.615393\n",
      "iteration 17360: loss 0.615393\n",
      "iteration 17370: loss 0.615393\n",
      "iteration 17380: loss 0.615393\n",
      "iteration 17390: loss 0.615393\n",
      "iteration 17400: loss 0.615393\n",
      "iteration 17410: loss 0.615393\n",
      "iteration 17420: loss 0.615393\n",
      "iteration 17430: loss 0.615393\n",
      "iteration 17440: loss 0.615393\n",
      "iteration 17450: loss 0.615393\n",
      "iteration 17460: loss 0.615393\n",
      "iteration 17470: loss 0.615393\n",
      "iteration 17480: loss 0.615393\n",
      "iteration 17490: loss 0.615393\n",
      "iteration 17500: loss 0.615393\n",
      "iteration 17510: loss 0.615393\n",
      "iteration 17520: loss 0.615393\n",
      "iteration 17530: loss 0.615393\n",
      "iteration 17540: loss 0.615393\n",
      "iteration 17550: loss 0.615393\n",
      "iteration 17560: loss 0.615393\n",
      "iteration 17570: loss 0.615393\n",
      "iteration 17580: loss 0.615393\n",
      "iteration 17590: loss 0.615393\n",
      "iteration 17600: loss 0.615393\n",
      "iteration 17610: loss 0.615393\n",
      "iteration 17620: loss 0.615393\n",
      "iteration 17630: loss 0.615393\n",
      "iteration 17640: loss 0.615393\n",
      "iteration 17650: loss 0.615393\n",
      "iteration 17660: loss 0.615393\n",
      "iteration 17670: loss 0.615393\n",
      "iteration 17680: loss 0.615393\n",
      "iteration 17690: loss 0.615393\n",
      "iteration 17700: loss 0.615393\n",
      "iteration 17710: loss 0.615393\n",
      "iteration 17720: loss 0.615393\n",
      "iteration 17730: loss 0.615393\n",
      "iteration 17740: loss 0.615393\n",
      "iteration 17750: loss 0.615393\n",
      "iteration 17760: loss 0.615393\n",
      "iteration 17770: loss 0.615393\n",
      "iteration 17780: loss 0.615393\n",
      "iteration 17790: loss 0.615393\n",
      "iteration 17800: loss 0.615393\n",
      "iteration 17810: loss 0.615393\n",
      "iteration 17820: loss 0.615393\n",
      "iteration 17830: loss 0.615393\n",
      "iteration 17840: loss 0.615393\n",
      "iteration 17850: loss 0.615393\n",
      "iteration 17860: loss 0.615393\n",
      "iteration 17870: loss 0.615393\n",
      "iteration 17880: loss 0.615393\n",
      "iteration 17890: loss 0.615393\n",
      "iteration 17900: loss 0.615393\n",
      "iteration 17910: loss 0.615393\n",
      "iteration 17920: loss 0.615393\n",
      "iteration 17930: loss 0.615393\n",
      "iteration 17940: loss 0.615393\n",
      "iteration 17950: loss 0.615393\n",
      "iteration 17960: loss 0.615393\n",
      "iteration 17970: loss 0.615393\n",
      "iteration 17980: loss 0.615393\n",
      "iteration 17990: loss 0.615393\n",
      "iteration 18000: loss 0.615393\n",
      "iteration 18010: loss 0.615393\n",
      "iteration 18020: loss 0.615393\n",
      "iteration 18030: loss 0.615393\n",
      "iteration 18040: loss 0.615393\n",
      "iteration 18050: loss 0.615393\n",
      "iteration 18060: loss 0.615393\n",
      "iteration 18070: loss 0.615393\n",
      "iteration 18080: loss 0.615393\n",
      "iteration 18090: loss 0.615393\n",
      "iteration 18100: loss 0.615393\n",
      "iteration 18110: loss 0.615393\n",
      "iteration 18120: loss 0.615393\n",
      "iteration 18130: loss 0.615393\n",
      "iteration 18140: loss 0.615393\n",
      "iteration 18150: loss 0.615393\n",
      "iteration 18160: loss 0.615393\n",
      "iteration 18170: loss 0.615393\n",
      "iteration 18180: loss 0.615393\n",
      "iteration 18190: loss 0.615393\n",
      "iteration 18200: loss 0.615393\n",
      "iteration 18210: loss 0.615393\n",
      "iteration 18220: loss 0.615393\n",
      "iteration 18230: loss 0.615393\n",
      "iteration 18240: loss 0.615393\n",
      "iteration 18250: loss 0.615393\n",
      "iteration 18260: loss 0.615393\n",
      "iteration 18270: loss 0.615393\n",
      "iteration 18280: loss 0.615393\n",
      "iteration 18290: loss 0.615393\n",
      "iteration 18300: loss 0.615393\n",
      "iteration 18310: loss 0.615393\n",
      "iteration 18320: loss 0.615393\n",
      "iteration 18330: loss 0.615393\n",
      "iteration 18340: loss 0.615393\n",
      "iteration 18350: loss 0.615393\n",
      "iteration 18360: loss 0.615393\n",
      "iteration 18370: loss 0.615393\n",
      "iteration 18380: loss 0.615393\n",
      "iteration 18390: loss 0.615393\n",
      "iteration 18400: loss 0.615393\n",
      "iteration 18410: loss 0.615393\n",
      "iteration 18420: loss 0.615393\n",
      "iteration 18430: loss 0.615393\n",
      "iteration 18440: loss 0.615393\n",
      "iteration 18450: loss 0.615393\n",
      "iteration 18460: loss 0.615393\n",
      "iteration 18470: loss 0.615393\n",
      "iteration 18480: loss 0.615393\n",
      "iteration 18490: loss 0.615393\n",
      "iteration 18500: loss 0.615393\n",
      "iteration 18510: loss 0.615393\n",
      "iteration 18520: loss 0.615393\n",
      "iteration 18530: loss 0.615393\n",
      "iteration 18540: loss 0.615393\n",
      "iteration 18550: loss 0.615393\n",
      "iteration 18560: loss 0.615393\n",
      "iteration 18570: loss 0.615393\n",
      "iteration 18580: loss 0.615393\n",
      "iteration 18590: loss 0.615393\n",
      "iteration 18600: loss 0.615393\n",
      "iteration 18610: loss 0.615393\n",
      "iteration 18620: loss 0.615393\n",
      "iteration 18630: loss 0.615393\n",
      "iteration 18640: loss 0.615393\n",
      "iteration 18650: loss 0.615393\n",
      "iteration 18660: loss 0.615393\n",
      "iteration 18670: loss 0.615393\n",
      "iteration 18680: loss 0.615393\n",
      "iteration 18690: loss 0.615393\n",
      "iteration 18700: loss 0.615393\n",
      "iteration 18710: loss 0.615393\n",
      "iteration 18720: loss 0.615393\n",
      "iteration 18730: loss 0.615393\n",
      "iteration 18740: loss 0.615393\n",
      "iteration 18750: loss 0.615393\n",
      "iteration 18760: loss 0.615393\n",
      "iteration 18770: loss 0.615393\n",
      "iteration 18780: loss 0.615393\n",
      "iteration 18790: loss 0.615393\n",
      "iteration 18800: loss 0.615393\n",
      "iteration 18810: loss 0.615393\n",
      "iteration 18820: loss 0.615393\n",
      "iteration 18830: loss 0.615393\n",
      "iteration 18840: loss 0.615393\n",
      "iteration 18850: loss 0.615393\n",
      "iteration 18860: loss 0.615393\n",
      "iteration 18870: loss 0.615393\n",
      "iteration 18880: loss 0.615393\n",
      "iteration 18890: loss 0.615393\n",
      "iteration 18900: loss 0.615393\n",
      "iteration 18910: loss 0.615393\n",
      "iteration 18920: loss 0.615393\n",
      "iteration 18930: loss 0.615393\n",
      "iteration 18940: loss 0.615393\n",
      "iteration 18950: loss 0.615393\n",
      "iteration 18960: loss 0.615393\n",
      "iteration 18970: loss 0.615393\n",
      "iteration 18980: loss 0.615393\n",
      "iteration 18990: loss 0.615393\n",
      "iteration 19000: loss 0.615393\n",
      "iteration 19010: loss 0.615393\n",
      "iteration 19020: loss 0.615393\n",
      "iteration 19030: loss 0.615393\n",
      "iteration 19040: loss 0.615393\n",
      "iteration 19050: loss 0.615393\n",
      "iteration 19060: loss 0.615393\n",
      "iteration 19070: loss 0.615393\n",
      "iteration 19080: loss 0.615393\n",
      "iteration 19090: loss 0.615393\n",
      "iteration 19100: loss 0.615393\n",
      "iteration 19110: loss 0.615393\n",
      "iteration 19120: loss 0.615393\n",
      "iteration 19130: loss 0.615393\n",
      "iteration 19140: loss 0.615393\n",
      "iteration 19150: loss 0.615393\n",
      "iteration 19160: loss 0.615393\n",
      "iteration 19170: loss 0.615393\n",
      "iteration 19180: loss 0.615393\n",
      "iteration 19190: loss 0.615393\n",
      "iteration 19200: loss 0.615393\n",
      "iteration 19210: loss 0.615393\n",
      "iteration 19220: loss 0.615393\n",
      "iteration 19230: loss 0.615393\n",
      "iteration 19240: loss 0.615393\n",
      "iteration 19250: loss 0.615393\n",
      "iteration 19260: loss 0.615393\n",
      "iteration 19270: loss 0.615393\n",
      "iteration 19280: loss 0.615393\n",
      "iteration 19290: loss 0.615393\n",
      "iteration 19300: loss 0.615393\n",
      "iteration 19310: loss 0.615393\n",
      "iteration 19320: loss 0.615393\n",
      "iteration 19330: loss 0.615393\n",
      "iteration 19340: loss 0.615393\n",
      "iteration 19350: loss 0.615393\n",
      "iteration 19360: loss 0.615393\n",
      "iteration 19370: loss 0.615393\n",
      "iteration 19380: loss 0.615393\n",
      "iteration 19390: loss 0.615393\n",
      "iteration 19400: loss 0.615393\n",
      "iteration 19410: loss 0.615393\n",
      "iteration 19420: loss 0.615393\n",
      "iteration 19430: loss 0.615393\n",
      "iteration 19440: loss 0.615393\n",
      "iteration 19450: loss 0.615393\n",
      "iteration 19460: loss 0.615393\n",
      "iteration 19470: loss 0.615393\n",
      "iteration 19480: loss 0.615393\n",
      "iteration 19490: loss 0.615393\n",
      "iteration 19500: loss 0.615393\n",
      "iteration 19510: loss 0.615393\n",
      "iteration 19520: loss 0.615393\n",
      "iteration 19530: loss 0.615393\n",
      "iteration 19540: loss 0.615393\n",
      "iteration 19550: loss 0.615393\n",
      "iteration 19560: loss 0.615393\n",
      "iteration 19570: loss 0.615393\n",
      "iteration 19580: loss 0.615393\n",
      "iteration 19590: loss 0.615393\n",
      "iteration 19600: loss 0.615393\n",
      "iteration 19610: loss 0.615393\n",
      "iteration 19620: loss 0.615393\n",
      "iteration 19630: loss 0.615393\n",
      "iteration 19640: loss 0.615393\n",
      "iteration 19650: loss 0.615393\n",
      "iteration 19660: loss 0.615393\n",
      "iteration 19670: loss 0.615393\n",
      "iteration 19680: loss 0.615393\n",
      "iteration 19690: loss 0.615393\n",
      "iteration 19700: loss 0.615393\n",
      "iteration 19710: loss 0.615393\n",
      "iteration 19720: loss 0.615393\n",
      "iteration 19730: loss 0.615393\n",
      "iteration 19740: loss 0.615393\n",
      "iteration 19750: loss 0.615393\n",
      "iteration 19760: loss 0.615393\n",
      "iteration 19770: loss 0.615393\n",
      "iteration 19780: loss 0.615393\n",
      "iteration 19790: loss 0.615393\n",
      "iteration 19800: loss 0.615393\n",
      "iteration 19810: loss 0.615393\n",
      "iteration 19820: loss 0.615393\n",
      "iteration 19830: loss 0.615393\n",
      "iteration 19840: loss 0.615393\n",
      "iteration 19850: loss 0.615393\n",
      "iteration 19860: loss 0.615393\n",
      "iteration 19870: loss 0.615393\n",
      "iteration 19880: loss 0.615393\n",
      "iteration 19890: loss 0.615393\n",
      "iteration 19900: loss 0.615393\n",
      "iteration 19910: loss 0.615393\n",
      "iteration 19920: loss 0.615393\n",
      "iteration 19930: loss 0.615393\n",
      "iteration 19940: loss 0.615393\n",
      "iteration 19950: loss 0.615393\n",
      "iteration 19960: loss 0.615393\n",
      "iteration 19970: loss 0.615393\n",
      "iteration 19980: loss 0.615393\n",
      "iteration 19990: loss 0.615393\n"
     ]
    }
   ],
   "source": [
    "#Train a Linear Classifier\n",
    "\n",
    "# initialize parameters randomly\n",
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "for i in xrange(20000):\n",
    "  \n",
    "  # evaluate class scores, [N x K]\n",
    "  scores = np.dot(X, W) + b \n",
    "  \n",
    "  # compute the class probabilities\n",
    "  exp_scores = np.exp(scores)\n",
    "  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "  \n",
    "  # compute the loss: average cross-entropy loss and regularization\n",
    "  corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "  data_loss = np.sum(corect_logprobs)/num_examples\n",
    "  reg_loss = 0.5*reg*np.sum(W*W)\n",
    "  loss = data_loss + reg_loss\n",
    "  if i % 10 == 0:\n",
    "    print \"iteration %d: loss %f\" % (i, loss)\n",
    "  \n",
    "  # compute the gradient on scores\n",
    "  dscores = probs\n",
    "  dscores[range(num_examples),y] -= 1\n",
    "  dscores /= num_examples\n",
    "  \n",
    "  # backpropate the gradient to the parameters (W,b)\n",
    "  dW = np.dot(X.T, dscores)\n",
    "  db = np.sum(dscores, axis=0, keepdims=True)\n",
    "  \n",
    "  dW += reg*W # regularization gradient\n",
    "  \n",
    "  # perform a parameter update\n",
    "  W += -step_size * dW\n",
    "  b += -step_size * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "# evaluate training set accuracy\n",
    "scores = np.dot(X, W) + b\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print 'training accuracy: %.2f' % (np.mean(predicted_class == y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.9800000000000026)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAHaCAYAAADc9jeSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a007650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the resulting classifier\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = np.dot(np.c_[xx.ravel(), yy.ravel()], W) + b\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=100)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "#fig.savefig('spiral_linear.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.693002\n",
      "iteration 1000: loss 0.193088\n",
      "iteration 2000: loss 0.144035\n",
      "iteration 3000: loss 0.227156\n",
      "iteration 4000: loss 0.149247\n",
      "iteration 5000: loss 0.216681\n",
      "iteration 6000: loss 0.247409\n",
      "iteration 7000: loss 0.153144\n",
      "iteration 8000: loss 0.216218\n",
      "iteration 9000: loss 0.233516\n",
      "iteration 10000: loss 0.151323\n",
      "iteration 11000: loss 0.214429\n",
      "iteration 12000: loss 0.170352\n",
      "iteration 13000: loss 0.169951\n",
      "iteration 14000: loss 0.173001\n",
      "iteration 15000: loss 0.191361\n",
      "iteration 16000: loss 0.245234\n",
      "iteration 17000: loss 0.213404\n",
      "iteration 18000: loss 0.145761\n",
      "iteration 19000: loss 0.163110\n",
      "iteration 20000: loss 0.151555\n",
      "iteration 21000: loss 0.140317\n",
      "iteration 22000: loss 0.278013\n",
      "iteration 23000: loss 0.271753\n",
      "iteration 24000: loss 0.178836\n",
      "iteration 25000: loss 0.208159\n",
      "iteration 26000: loss 0.169931\n",
      "iteration 27000: loss 0.157986\n",
      "iteration 28000: loss 0.258578\n",
      "iteration 29000: loss 0.282604\n",
      "iteration 30000: loss 0.178059\n",
      "iteration 31000: loss 0.169572\n",
      "iteration 32000: loss 0.220675\n",
      "iteration 33000: loss 0.181694\n",
      "iteration 34000: loss 0.265579\n",
      "iteration 35000: loss 0.245554\n",
      "iteration 36000: loss 0.135086\n",
      "iteration 37000: loss 0.198716\n",
      "iteration 38000: loss 0.243301\n",
      "iteration 39000: loss 0.211403\n",
      "iteration 40000: loss 0.429187\n",
      "iteration 41000: loss 0.233367\n",
      "iteration 42000: loss 0.208857\n",
      "iteration 43000: loss 0.266391\n",
      "iteration 44000: loss 0.191192\n",
      "iteration 45000: loss 0.154625\n",
      "iteration 46000: loss 0.797870\n",
      "iteration 47000: loss 0.156718\n",
      "iteration 48000: loss 0.149226\n",
      "iteration 49000: loss 0.229507\n",
      "iteration 50000: loss 0.215718\n",
      "iteration 51000: loss 0.155861\n",
      "iteration 52000: loss 0.206607\n",
      "iteration 53000: loss 0.171152\n",
      "iteration 54000: loss 0.262503\n",
      "iteration 55000: loss 0.257753\n",
      "iteration 56000: loss 0.191593\n",
      "iteration 57000: loss 0.250487\n",
      "iteration 58000: loss 0.206811\n",
      "iteration 59000: loss 0.240504\n",
      "iteration 60000: loss 0.234931\n",
      "iteration 61000: loss 0.223831\n",
      "iteration 62000: loss 0.232003\n",
      "iteration 63000: loss 0.201054\n",
      "iteration 64000: loss 0.220511\n",
      "iteration 65000: loss 0.210297\n",
      "iteration 66000: loss 0.229326\n",
      "iteration 67000: loss 0.278027\n",
      "iteration 68000: loss 0.217021\n",
      "iteration 69000: loss 0.366383\n",
      "iteration 70000: loss 0.235107\n",
      "iteration 71000: loss 0.245948\n",
      "iteration 72000: loss 0.233231\n",
      "iteration 73000: loss 0.182645\n",
      "iteration 74000: loss 0.202867\n",
      "iteration 75000: loss 0.222061\n",
      "iteration 76000: loss 0.189221\n",
      "iteration 77000: loss 0.161691\n",
      "iteration 78000: loss 0.212994\n",
      "iteration 79000: loss 0.156602\n",
      "iteration 80000: loss 0.230068\n",
      "iteration 81000: loss 0.196456\n",
      "iteration 82000: loss 0.164430\n",
      "iteration 83000: loss 0.204524\n",
      "iteration 84000: loss 0.184092\n",
      "iteration 85000: loss 0.152453\n",
      "iteration 86000: loss 0.212247\n",
      "iteration 87000: loss 0.164839\n",
      "iteration 88000: loss 0.240897\n",
      "iteration 89000: loss 0.141136\n",
      "iteration 90000: loss 0.218558\n",
      "iteration 91000: loss 0.608010\n",
      "iteration 92000: loss 0.243723\n",
      "iteration 93000: loss 0.145108\n",
      "iteration 94000: loss 0.184085\n",
      "iteration 95000: loss 0.173900\n",
      "iteration 96000: loss 0.236358\n",
      "iteration 97000: loss 0.224047\n",
      "iteration 98000: loss 0.226128\n",
      "iteration 99000: loss 0.247608\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters randomly\n",
    "h = 100 # size of hidden layer\n",
    "W = 0.01 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "W2 = 0.01 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e-0\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "for i in xrange(100000):\n",
    "  \n",
    "  # evaluate class scores, [N x K]\n",
    "  hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n",
    "  scores = np.dot(hidden_layer, W2) + b2\n",
    "  \n",
    "  # compute the class probabilities\n",
    "  exp_scores = np.exp(scores)\n",
    "  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "  \n",
    "  # compute the loss: average cross-entropy loss and regularization\n",
    "  corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "  data_loss = np.sum(corect_logprobs)/num_examples\n",
    "  reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "  loss = data_loss + reg_loss\n",
    "  if i % 1000 == 0:\n",
    "    print \"iteration %d: loss %f\" % (i, loss)\n",
    "  \n",
    "  # compute the gradient on scores\n",
    "  dscores = probs\n",
    "  dscores[range(num_examples),y] -= 1\n",
    "  dscores /= num_examples\n",
    "  \n",
    "  # backpropate the gradient to the parameters\n",
    "  # first backprop into parameters W2 and b2\n",
    "  dW2 = np.dot(hidden_layer.T, dscores)\n",
    "  db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "  # next backprop into hidden layer\n",
    "  dhidden = np.dot(dscores, W2.T)\n",
    "  # backprop the ReLU non-linearity\n",
    "  dhidden[hidden_layer <= 0] = 0\n",
    "  # finally into W,b\n",
    "  dW = np.dot(X.T, dhidden)\n",
    "  db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "  \n",
    "  # add regularization gradient contribution\n",
    "  dW2 += reg * W2\n",
    "  dW += reg * W\n",
    "  \n",
    "  # perform a parameter update\n",
    "  W += -step_size * dW\n",
    "  b += -step_size * db\n",
    "  W2 += -step_size * dW2\n",
    "  b2 += -step_size * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# evaluate training set accuracy\n",
    "hidden_layer = np.maximum(0, np.dot(X, W) + b)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print 'training accuracy: %.2f' % (np.mean(predicted_class == y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.9800000000000026)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAHaCAYAAADc9jeSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e3f7450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the resulting classifier\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = np.dot(np.maximum(0, np.dot(np.c_[xx.ravel(), yy.ravel()], W) + b), W2) + b2\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=100)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "#fig.savefig('spiral_net.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
